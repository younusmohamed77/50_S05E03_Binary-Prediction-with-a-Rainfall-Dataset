{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fd0e0fe",
   "metadata": {
    "papermill": {
     "duration": 0.003938,
     "end_time": "2025-03-11T13:46:14.180953",
     "exception": false,
     "start_time": "2025-03-11T13:46:14.177015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cell 0: Check for GPU Availability\n",
    "\n",
    "Ensure that TensorFlow detects one or more GPUs. Run this cell in a GPU-enabled environment (e.g., Kaggle Kernels with GPU).\n",
    ".\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "375561c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T13:46:14.188487Z",
     "iopub.status.busy": "2025-03-11T13:46:14.188258Z",
     "iopub.status.idle": "2025-03-11T13:46:27.904554Z",
     "shell.execute_reply": "2025-03-11T13:46:27.903469Z"
    },
    "papermill": {
     "duration": 13.721644,
     "end_time": "2025-03-11T13:46:27.906115",
     "exception": false,
     "start_time": "2025-03-11T13:46:14.184471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ebd79",
   "metadata": {
    "papermill": {
     "duration": 0.003431,
     "end_time": "2025-03-11T13:46:27.913386",
     "exception": false,
     "start_time": "2025-03-11T13:46:27.909955",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Libraries\n",
    "\n",
    "We import necessary libraries including scikeras for wrapping our Keras model, and also import SimpleImputer (for missing value imputation). GPU-enabled training will be used if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9b9f5e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T13:46:27.921447Z",
     "iopub.status.busy": "2025-03-11T13:46:27.920987Z",
     "iopub.status.idle": "2025-03-11T13:46:36.085363Z",
     "shell.execute_reply": "2025-03-11T13:46:36.084168Z"
    },
    "papermill": {
     "duration": 8.170275,
     "end_time": "2025-03-11T13:46:36.087101",
     "exception": false,
     "start_time": "2025-03-11T13:46:27.916826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scikeras --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5b729fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T13:46:36.096231Z",
     "iopub.status.busy": "2025-03-11T13:46:36.095948Z",
     "iopub.status.idle": "2025-03-11T13:46:41.218912Z",
     "shell.execute_reply": "2025-03-11T13:46:41.217577Z"
    },
    "papermill": {
     "duration": 5.129564,
     "end_time": "2025-03-11T13:46:41.220982",
     "exception": false,
     "start_time": "2025-03-11T13:46:36.091418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, time\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Install scikeras if needed\n",
    "!pip install scikeras --quiet\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a63aaa",
   "metadata": {
    "papermill": {
     "duration": 0.004634,
     "end_time": "2025-03-11T13:46:41.230599",
     "exception": false,
     "start_time": "2025-03-11T13:46:41.225965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Memory Optimization Function\r\n",
    "\r\n",
    "This function attempts to reduce memory usage by downcasting numeric columns to smaller data types (e.g., float64 -> float32, int64 -> int32.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba8836bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T13:46:41.240991Z",
     "iopub.status.busy": "2025-03-11T13:46:41.239992Z",
     "iopub.status.idle": "2025-03-11T13:46:41.246733Z",
     "shell.execute_reply": "2025-03-11T13:46:41.245790Z"
    },
    "papermill": {
     "duration": 0.013225,
     "end_time": "2025-03-11T13:46:41.248160",
     "exception": false,
     "start_time": "2025-03-11T13:46:41.234935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define memory optimization function\n",
    "def reduce_memory_usage(df, verbose=True):\n",
    "    \"\"\"Downcasts numeric columns to reduce memory usage.\"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type == 'float64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        elif col_type == 'int64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f\"Memory usage reduced from {start_mem:.2f} MB to {end_mem:.2f} MB \"\n",
    "              f\"({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7dac73",
   "metadata": {
    "papermill": {
     "duration": 0.004078,
     "end_time": "2025-03-11T13:46:41.256628",
     "exception": false,
     "start_time": "2025-03-11T13:46:41.252550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load and Optimize Data\r\n",
    "\r\n",
    "We load our train/test data, apply the memory optimization function, and separate features/target. Adjust file names as needed for your environmen.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "244f87a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T13:46:41.266021Z",
     "iopub.status.busy": "2025-03-11T13:46:41.265699Z",
     "iopub.status.idle": "2025-03-11T13:46:41.338327Z",
     "shell.execute_reply": "2025-03-11T13:46:41.337454Z"
    },
    "papermill": {
     "duration": 0.079062,
     "end_time": "2025-03-11T13:46:41.339879",
     "exception": false,
     "start_time": "2025-03-11T13:46:41.260817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before memory optimization:\n",
      "Train shape: (2190, 13)\n",
      "Test shape: (730, 12)\n",
      "Memory usage reduced from 0.22 MB to 0.09 MB (56.7% reduction)\n",
      "Memory usage reduced from 0.07 MB to 0.03 MB (54.1% reduction)\n",
      "After optimization:\n",
      "X shape: (2190, 11)\n",
      "X_test shape: (730, 11)\n"
     ]
    }
   ],
   "source": [
    "# Load and optimize data\n",
    "train = pd.read_csv('/kaggle/input/playground-series-s5e3/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/playground-series-s5e3/test.csv')\n",
    "\n",
    "print(\"Before memory optimization:\")\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "\n",
    "train = reduce_memory_usage(train)\n",
    "test = reduce_memory_usage(test)\n",
    "\n",
    "# Separate features and target from training data\n",
    "X = train.drop(columns=['id', 'rainfall'])\n",
    "y = train['rainfall']\n",
    "\n",
    "# For test data, drop the id column and save the IDs for submission\n",
    "X_test = test.drop(columns=['id'])\n",
    "test_ids = test['id']\n",
    "\n",
    "print(\"After optimization:\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa42b87b",
   "metadata": {
    "papermill": {
     "duration": 0.003999,
     "end_time": "2025-03-11T13:46:41.348427",
     "exception": false,
     "start_time": "2025-03-11T13:46:41.344428",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Preprocessing Pipelines\r\n",
    "\r\n",
    "We create a dictionary of scalers to compare (none, standard, minmax, robust). If you prefer, you can remove or add more pipeline.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baf84e3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T13:46:41.357574Z",
     "iopub.status.busy": "2025-03-11T13:46:41.357328Z",
     "iopub.status.idle": "2025-03-11T13:46:41.360686Z",
     "shell.execute_reply": "2025-03-11T13:46:41.360057Z"
    },
    "papermill": {
     "duration": 0.009517,
     "end_time": "2025-03-11T13:46:41.362119",
     "exception": false,
     "start_time": "2025-03-11T13:46:41.352602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define preprocessing pipelines\n",
    "preprocessors = {\n",
    "    'standard': StandardScaler(),\n",
    "    'minmax': MinMaxScaler(),\n",
    "    'robust': RobustScaler(),\n",
    "    'raw': None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425de289",
   "metadata": {
    "papermill": {
     "duration": 0.003944,
     "end_time": "2025-03-11T13:46:41.370344",
     "exception": false,
     "start_time": "2025-03-11T13:46:41.366400",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Modified : Define the Expanded Keras DNN Model Function\n",
    "\n",
    "This version of the model function accepts extra hyperparameters to allow for a deeper network. If `num_layers` is 2, the model will have two hidden layers; if 3, then three hidden layers are added..\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab6a4aa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T13:46:41.379662Z",
     "iopub.status.busy": "2025-03-11T13:46:41.379427Z",
     "iopub.status.idle": "2025-03-11T13:46:41.384764Z",
     "shell.execute_reply": "2025-03-11T13:46:41.384059Z"
    },
    "papermill": {
     "duration": 0.011586,
     "end_time": "2025-03-11T13:46:41.386144",
     "exception": false,
     "start_time": "2025-03-11T13:46:41.374558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modified : Define the expanded Keras model function\n",
    "def create_dnn_model(optimizer='adam', dropout_rate=0.2, learning_rate=0.001,\n",
    "                     num_layers=2, units1=64, units2=32, units3=16):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units1, activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(units2, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    if num_layers == 3:\n",
    "        model.add(Dense(units3, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Build optimizer with the specified learning rate\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0a7b45",
   "metadata": {
    "papermill": {
     "duration": 0.003951,
     "end_time": "2025-03-11T13:46:41.394193",
     "exception": false,
     "start_time": "2025-03-11T13:46:41.390242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Modified : Update KerasClassifier and Expanded Hyperparameter Grid\n",
    "\n",
    "Note that since our pipeline step is named `'dnn'`, the parameters for the model-building function use the prefix `dnn__model__`. In addition, we add new hyperparameters: `num_layers`, `units1`, `units2`, and `units3`..\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8f774c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T13:46:41.403678Z",
     "iopub.status.busy": "2025-03-11T13:46:41.403422Z",
     "iopub.status.idle": "2025-03-11T13:46:41.407669Z",
     "shell.execute_reply": "2025-03-11T13:46:41.406872Z"
    },
    "papermill": {
     "duration": 0.010471,
     "end_time": "2025-03-11T13:46:41.409176",
     "exception": false,
     "start_time": "2025-03-11T13:46:41.398705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modified : Wrap the Keras model and define an expanded hyperparameter grid\n",
    "dnn_wrapper = KerasClassifier(\n",
    "    model=create_dnn_model,\n",
    "    epochs=20,           # Default; will be tuned\n",
    "    batch_size=32,       # Default; will be tuned\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Expanded hyperparameter grid\n",
    "param_dist = {\n",
    "    'dnn__model__optimizer': ['adam', 'rmsprop'],\n",
    "    'dnn__model__dropout_rate': [0.2, 0.3, 0.4],\n",
    "    'dnn__model__learning_rate': [1e-3, 1e-4, 5e-4],\n",
    "    'dnn__epochs': [20, 30],\n",
    "    'dnn__batch_size': [32, 64],\n",
    "    'dnn__model__num_layers': [2, 3],\n",
    "    'dnn__model__units1': [64, 128],\n",
    "    'dnn__model__units2': [32, 64],\n",
    "    # units3 will be used only if num_layers==3. We include it in grid.\n",
    "    'dnn__model__units3': [16, 32]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb35165",
   "metadata": {
    "papermill": {
     "duration": 0.003521,
     "end_time": "2025-03-11T13:46:41.417084",
     "exception": false,
     "start_time": "2025-03-11T13:46:41.413563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run Hyperparameter Tuning and Generate Submissions for All Preprocessing Methods\n",
    "\n",
    "This cell remains similar to before. For each preprocessing method (Standard, MinMax, Robust, Raw), we build a pipeline that includes an imputer, the scaler (if any), and the DNN. We run RandomizedSearchCV (using a threading backend) to tune hyperparameters. Then we manually transform the test data (to avoid the __sklearn_tags__ error) and call `predict_proba` on the DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f9e771d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T13:46:41.425376Z",
     "iopub.status.busy": "2025-03-11T13:46:41.425139Z"
    },
    "papermill": {
     "duration": 29380.04761,
     "end_time": "2025-03-11T21:56:21.468397",
     "exception": false,
     "start_time": "2025-03-11T13:46:41.420787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing: standard ---\n",
      "Best CV ROC AUC for standard: nan\n",
      "Best parameters: {'dnn__model__units3': 16, 'dnn__model__units2': 64, 'dnn__model__units1': 128, 'dnn__model__optimizer': 'rmsprop', 'dnn__model__num_layers': 2, 'dnn__model__learning_rate': 0.0005, 'dnn__model__dropout_rate': 0.3, 'dnn__epochs': 20, 'dnn__batch_size': 32}\n",
      "Submission file saved: submissions/standard_dnn_submission.csv\n",
      "\n",
      "\n",
      "--- Preprocessing: minmax ---\n"
     ]
    }
   ],
   "source": [
    "# Modified Cell 7: Hyperparameter tuning and submission generation (all preprocessing methods)\n",
    "from joblib import parallel_backend\n",
    "\n",
    "results = []  # List to store tuning results\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "\n",
    "for prep_name, scaler in preprocessors.items():\n",
    "    print(f\"\\n--- Preprocessing: {prep_name} ---\")\n",
    "    \n",
    "    # Build pipeline: imputer -> (scaler if provided) -> DNN\n",
    "    steps = []\n",
    "    steps.append(('imputer', SimpleImputer(strategy=\"median\")))\n",
    "    if scaler is not None:\n",
    "        steps.append((prep_name, scaler))\n",
    "    steps.append(('dnn', dnn_wrapper))\n",
    "    pipeline = Pipeline(steps)\n",
    "    \n",
    "    with parallel_backend('threading', n_jobs=-1):\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=100,  # Increase iterations for a more thorough search\n",
    "            scoring='roc_auc',\n",
    "            cv=cv,\n",
    "            verbose=0,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        search.fit(X, y)\n",
    "    \n",
    "    best_score = search.best_score_\n",
    "    best_params = search.best_params_\n",
    "    \n",
    "    print(f\"Best CV ROC AUC for {prep_name}: {best_score:.4f}\")\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    \n",
    "    # Re-fit best estimator on full training data\n",
    "    best_estimator = search.best_estimator_\n",
    "    best_estimator.fit(X, y)\n",
    "    \n",
    "    # Manually transform X_test using all steps except the final DNN step, then predict\n",
    "    X_test_transformed = best_estimator[:-1].transform(X_test)\n",
    "    test_preds = best_estimator.named_steps['dnn'].predict_proba(X_test_transformed)[:, 1]\n",
    "    \n",
    "    # Save submission file for this preprocessing method\n",
    "    sub_filename = f\"submissions/{prep_name}_dnn_submission.csv\"\n",
    "    sub_df = pd.DataFrame({'id': test_ids, 'rainfall': test_preds})\n",
    "    sub_df.to_csv(sub_filename, index=False)\n",
    "    print(f\"Submission file saved: {sub_filename}\\n\")\n",
    "    \n",
    "    results.append({\n",
    "        'preprocessing': prep_name,\n",
    "        'best_auc': best_score,\n",
    "        'best_params': best_params,\n",
    "        'submission_file': sub_filename\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b691037",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Graphical Comparison of CV ROC AUC Scores\n",
    "\n",
    "We create a bar plot comparing the best CV ROC AUC scores for each preprocessing method..\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef464ad1",
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-11T13:46:00.135Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a bar plot to compare best CV ROC AUC scores\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='best_auc', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x='preprocessing', y='best_auc', data=results_df, palette='viridis')\n",
    "plt.title(\"Best CV ROC AUC by Preprocessing Method\")\n",
    "plt.xlabel(\"Preprocessing Method\")\n",
    "plt.ylabel(\"Best CV ROC AUC\")\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be8be2e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "- **Why Was the Score NaN?**  \n",
    "  The NaN ROC AUC score may occur if certain hyperparameter combinations cause unstable training (e.g., a too-high learning rate, improper network architecture, or other settings that cause the model’s loss to become NaN or its predictions to be constant). Expanding the hyperparameter search with additional layers and units gives the model more capacity and can help stabilize training.\n",
    "\n",
    "- **Expanded Hyperparameter Search:**  \n",
    "  We increased the search space to include the number of layers (2 or 3) and varied the number of neurons in each layer. Additionally, we expanded the ranges for dropout rates and learning rates and increased the number of iterations for RandomizedSearchCV.\n",
    "\n",
    "- **Graphical Comparison:**  \n",
    "  A bar plot shows the best CV ROC AUC scores for each preprocessing method, so you can visually inspect which one performed best.\n",
    "\n",
    "- **Final Submission:**  \n",
    "  The highest scoring submission is automatically copied to **submission.csv**, while all individual submissions are saved in the \"submissions\" folder.\n",
    "\n",
    "This modified and expanded search is designed to improve model performance in a competitive setting. Happy modeling and best of luck in the competition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7b134",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11251744,
     "sourceId": 91714,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29411.044127,
   "end_time": "2025-03-11T21:56:22.492489",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-11T13:46:11.448362",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
