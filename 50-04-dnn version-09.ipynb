{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0154e2",
   "metadata": {
    "papermill": {
     "duration": 0.00409,
     "end_time": "2025-03-18T11:30:47.315796",
     "exception": false,
     "start_time": "2025-03-18T11:30:47.311706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cell 0: Check for GPU Availability\n",
    "\n",
    "Ensure that TensorFlow detects one or more GPUs. Run this cell in a GPU-enabled environment (e.g., Kaggle Kernels with GPU).\n",
    ".\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b3d90f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T11:30:47.323919Z",
     "iopub.status.busy": "2025-03-18T11:30:47.323612Z",
     "iopub.status.idle": "2025-03-18T11:31:00.364533Z",
     "shell.execute_reply": "2025-03-18T11:31:00.363433Z"
    },
    "papermill": {
     "duration": 13.046868,
     "end_time": "2025-03-18T11:31:00.366191",
     "exception": false,
     "start_time": "2025-03-18T11:30:47.319323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03056c4",
   "metadata": {
    "papermill": {
     "duration": 0.003579,
     "end_time": "2025-03-18T11:31:00.374214",
     "exception": false,
     "start_time": "2025-03-18T11:31:00.370635",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Libraries\n",
    "\n",
    "We import necessary libraries including scikeras for wrapping our Keras model, and also import SimpleImputer (for missing value imputation). GPU-enabled training will be used if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6d6410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T11:31:00.382758Z",
     "iopub.status.busy": "2025-03-18T11:31:00.382249Z",
     "iopub.status.idle": "2025-03-18T11:31:08.017786Z",
     "shell.execute_reply": "2025-03-18T11:31:08.016712Z"
    },
    "papermill": {
     "duration": 7.641704,
     "end_time": "2025-03-18T11:31:08.019460",
     "exception": false,
     "start_time": "2025-03-18T11:31:00.377756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scikeras --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6f3824",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T11:31:08.028271Z",
     "iopub.status.busy": "2025-03-18T11:31:08.028000Z",
     "iopub.status.idle": "2025-03-18T11:31:12.685695Z",
     "shell.execute_reply": "2025-03-18T11:31:12.684784Z"
    },
    "papermill": {
     "duration": 4.66368,
     "end_time": "2025-03-18T11:31:12.687312",
     "exception": false,
     "start_time": "2025-03-18T11:31:08.023632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, time\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Install scikeras if needed\n",
    "!pip install scikeras --quiet\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96387a43",
   "metadata": {
    "papermill": {
     "duration": 0.003769,
     "end_time": "2025-03-18T11:31:12.695170",
     "exception": false,
     "start_time": "2025-03-18T11:31:12.691401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Memory Optimization Function\r\n",
    "\r\n",
    "This function attempts to reduce memory usage by downcasting numeric columns to smaller data types (e.g., float64 -> float32, int64 -> int32.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb6f00ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T11:31:12.703752Z",
     "iopub.status.busy": "2025-03-18T11:31:12.702923Z",
     "iopub.status.idle": "2025-03-18T11:31:12.708412Z",
     "shell.execute_reply": "2025-03-18T11:31:12.707801Z"
    },
    "papermill": {
     "duration": 0.011177,
     "end_time": "2025-03-18T11:31:12.709868",
     "exception": false,
     "start_time": "2025-03-18T11:31:12.698691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define memory optimization function\n",
    "def reduce_memory_usage(df, verbose=True):\n",
    "    \"\"\"Downcasts numeric columns to reduce memory usage.\"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type == 'float64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        elif col_type == 'int64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f\"Memory usage reduced from {start_mem:.2f} MB to {end_mem:.2f} MB \"\n",
    "              f\"({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3601a48",
   "metadata": {
    "papermill": {
     "duration": 0.003372,
     "end_time": "2025-03-18T11:31:12.717758",
     "exception": false,
     "start_time": "2025-03-18T11:31:12.714386",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load and Optimize Data\r\n",
    "\r\n",
    "We load our train/test data, apply the memory optimization function, and separate features/target. Adjust file names as needed for your environmen.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8fd140e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T11:31:12.725644Z",
     "iopub.status.busy": "2025-03-18T11:31:12.725422Z",
     "iopub.status.idle": "2025-03-18T11:31:12.780168Z",
     "shell.execute_reply": "2025-03-18T11:31:12.779375Z"
    },
    "papermill": {
     "duration": 0.06013,
     "end_time": "2025-03-18T11:31:12.781446",
     "exception": false,
     "start_time": "2025-03-18T11:31:12.721316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before memory optimization:\n",
      "Train shape: (2190, 13)\n",
      "Test shape: (730, 12)\n",
      "Memory usage reduced from 0.22 MB to 0.09 MB (56.7% reduction)\n",
      "Memory usage reduced from 0.07 MB to 0.03 MB (54.1% reduction)\n",
      "After optimization:\n",
      "X shape: (2190, 11)\n",
      "X_test shape: (730, 11)\n"
     ]
    }
   ],
   "source": [
    "# Load and optimize data\n",
    "train = pd.read_csv('/kaggle/input/playground-series-s5e3/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/playground-series-s5e3/test.csv')\n",
    "\n",
    "print(\"Before memory optimization:\")\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "\n",
    "train = reduce_memory_usage(train)\n",
    "test = reduce_memory_usage(test)\n",
    "\n",
    "# Separate features and target from training data\n",
    "X = train.drop(columns=['id', 'rainfall'])\n",
    "y = train['rainfall']\n",
    "\n",
    "# For test data, drop the id column and save the IDs for submission\n",
    "X_test = test.drop(columns=['id'])\n",
    "test_ids = test['id']\n",
    "\n",
    "print(\"After optimization:\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d3b6f",
   "metadata": {
    "papermill": {
     "duration": 0.003435,
     "end_time": "2025-03-18T11:31:12.788753",
     "exception": false,
     "start_time": "2025-03-18T11:31:12.785318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Preprocessing Pipelines\r\n",
    "\r\n",
    "We create a dictionary of scalers to compare (none, standard, minmax, robust). If you prefer, you can remove or add more pipeline.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "511eb7ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T11:31:12.796644Z",
     "iopub.status.busy": "2025-03-18T11:31:12.796407Z",
     "iopub.status.idle": "2025-03-18T11:31:12.799438Z",
     "shell.execute_reply": "2025-03-18T11:31:12.798828Z"
    },
    "papermill": {
     "duration": 0.00834,
     "end_time": "2025-03-18T11:31:12.800662",
     "exception": false,
     "start_time": "2025-03-18T11:31:12.792322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define preprocessing pipelines\n",
    "preprocessors = {\n",
    "    'standard': StandardScaler(),\n",
    "    # 'minmax': MinMaxScaler(),\n",
    "    #'robust': RobustScaler(),\n",
    "    # 'raw': None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a39ea",
   "metadata": {
    "papermill": {
     "duration": 0.003356,
     "end_time": "2025-03-18T11:31:12.808809",
     "exception": false,
     "start_time": "2025-03-18T11:31:12.805453",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Modified : Define the Expanded Keras DNN Model Function\n",
    "\n",
    "This version of the model function accepts extra hyperparameters to allow for a deeper network. If `num_layers` is 2, the model will have two hidden layers; if 3, then three hidden layers are added..\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6883d830",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T11:31:12.816706Z",
     "iopub.status.busy": "2025-03-18T11:31:12.816451Z",
     "iopub.status.idle": "2025-03-18T11:31:12.821432Z",
     "shell.execute_reply": "2025-03-18T11:31:12.820781Z"
    },
    "papermill": {
     "duration": 0.010115,
     "end_time": "2025-03-18T11:31:12.822522",
     "exception": false,
     "start_time": "2025-03-18T11:31:12.812407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modified : Define the expanded Keras model function\n",
    "def create_dnn_model(optimizer='adam', dropout_rate=0.2, learning_rate=0.001,\n",
    "                     num_layers=3, units1=64, units2=32, units3=16):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units, activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776be5ca",
   "metadata": {
    "papermill": {
     "duration": 0.00346,
     "end_time": "2025-03-18T11:31:12.829619",
     "exception": false,
     "start_time": "2025-03-18T11:31:12.826159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Modified : Update KerasClassifier and Expanded Hyperparameter Grid\n",
    "\n",
    "Note that since our pipeline step is named `'dnn'`, the parameters for the model-building function use the prefix `dnn__model__`. In addition, we add new hyperparameters: `num_layers`, `units1`, `units2`, and `units3`..\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a56a4fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T11:31:12.837496Z",
     "iopub.status.busy": "2025-03-18T11:31:12.837273Z",
     "iopub.status.idle": "2025-03-18T11:31:12.841230Z",
     "shell.execute_reply": "2025-03-18T11:31:12.840629Z"
    },
    "papermill": {
     "duration": 0.009074,
     "end_time": "2025-03-18T11:31:12.842326",
     "exception": false,
     "start_time": "2025-03-18T11:31:12.833252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modified : Wrap the Keras model and define an expanded hyperparameter grid\n",
    "dnn_wrapper = KerasClassifier(\n",
    "    model=create_dnn_model,\n",
    "    epochs=20,           # Default; will be tuned\n",
    "    batch_size=32,       # Default; will be tuned\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Expanded hyperparameter grid\n",
    "param_dist = {\n",
    "    'dnn__model__optimizer': ['adam', 'rmsprop'],\n",
    "    'dnn__model__dropout_rate': [0.2, 0.3, 0.4],\n",
    "    'dnn__model__learning_rate': [1e-3, 1e-4, 5e-4],\n",
    "    'dnn__epochs': [20, 30, 50],\n",
    "    'dnn__batch_size': [16, 32, 64],\n",
    "    'dnn__model__num_layers': [2, 3, 4],\n",
    "    'dnn__model__units1': [32, 64, 128],\n",
    "    'dnn__model__units2': [32, 64, 128],\n",
    "    # units3 will be used only if num_layers==3. We include it in grid.\n",
    "    'dnn__model__units3': [16, 32, 64]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29111719",
   "metadata": {
    "papermill": {
     "duration": 0.003396,
     "end_time": "2025-03-18T11:31:12.849355",
     "exception": false,
     "start_time": "2025-03-18T11:31:12.845959",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run Hyperparameter Tuning and Generate Submissions for All Preprocessing Methods\n",
    "\n",
    "This cell remains similar to before. For each preprocessing method (Standard, MinMax, Robust, Raw), we build a pipeline that includes an imputer, the scaler (if any), and the DNN. We run RandomizedSearchCV (using a threading backend) to tune hyperparameters. Then we manually transform the test data (to avoid the __sklearn_tags__ error) and call `predict_proba` on the DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92cb38ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T11:31:12.857179Z",
     "iopub.status.busy": "2025-03-18T11:31:12.856960Z",
     "iopub.status.idle": "2025-03-18T11:31:19.443734Z",
     "shell.execute_reply": "2025-03-18T11:31:19.442451Z"
    },
    "papermill": {
     "duration": 6.591961,
     "end_time": "2025-03-18T11:31:19.444897",
     "exception": true,
     "start_time": "2025-03-18T11:31:12.852936",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing: standard ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 500 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n500 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 662, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py\", line 1501, in fit\n    super().fit(X=X, y=y, sample_weight=sample_weight, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py\", line 770, in fit\n    self._fit(\n  File \"/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py\", line 925, in _fit\n    X, y = self._initialize(X, y)\n  File \"/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py\", line 862, in _initialize\n    self.model_ = self._build_keras_model()\n  File \"/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py\", line 433, in _build_keras_model\n    model = final_build_fn(**build_params)\n  File \"<ipython-input-7-e5ce33b9d7e1>\", line 5, in create_dnn_model\n    model.add(Dense(units, activation='relu', input_shape=(X.shape[1],)))\nNameError: name 'units' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a7067b6d0c44>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         )\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1951\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1952\u001b[0m             ParameterSampler(\n\u001b[1;32m   1953\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    999\u001b[0m                     )\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m                 \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \u001b[0;31m# For callable self.scoring, the return type is only know after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             )\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 500 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n500 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 662, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py\", line 1501, in fit\n    super().fit(X=X, y=y, sample_weight=sample_weight, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py\", line 770, in fit\n    self._fit(\n  File \"/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py\", line 925, in _fit\n    X, y = self._initialize(X, y)\n  File \"/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py\", line 862, in _initialize\n    self.model_ = self._build_keras_model()\n  File \"/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py\", line 433, in _build_keras_model\n    model = final_build_fn(**build_params)\n  File \"<ipython-input-7-e5ce33b9d7e1>\", line 5, in create_dnn_model\n    model.add(Dense(units, activation='relu', input_shape=(X.shape[1],)))\nNameError: name 'units' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Modified Cell 7: Hyperparameter tuning and submission generation (all preprocessing methods)\n",
    "from joblib import parallel_backend\n",
    "\n",
    "results = []  # List to store tuning results\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "\n",
    "for prep_name, scaler in preprocessors.items():\n",
    "    print(f\"\\n--- Preprocessing: {prep_name} ---\")\n",
    "    \n",
    "    # Build pipeline: imputer -> (scaler if provided) -> DNN\n",
    "    steps = []\n",
    "    steps.append(('imputer', SimpleImputer(strategy=\"median\")))\n",
    "    if scaler is not None:\n",
    "        steps.append((prep_name, scaler))\n",
    "    steps.append(('dnn', dnn_wrapper))\n",
    "    pipeline = Pipeline(steps)\n",
    "    \n",
    "    with parallel_backend('threading', n_jobs=-1):\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=100,  # Increase iterations for a more thorough search\n",
    "            scoring='roc_auc',\n",
    "            cv=cv,\n",
    "            verbose=0,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        search.fit(X, y)\n",
    "    \n",
    "    best_score = search.best_score_\n",
    "    best_params = search.best_params_\n",
    "    \n",
    "    print(f\"Best CV ROC AUC for {prep_name}: {best_score:.4f}\")\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    \n",
    "    # Re-fit best estimator on full training data\n",
    "    best_estimator = search.best_estimator_\n",
    "    best_estimator.fit(X, y)\n",
    "    \n",
    "    # Manually transform X_test using all steps except the final DNN step, then predict\n",
    "    X_test_transformed = best_estimator[:-1].transform(X_test)\n",
    "    test_preds = best_estimator.named_steps['dnn'].predict_proba(X_test_transformed)[:, 1]\n",
    "    \n",
    "    # Save submission file for this preprocessing method\n",
    "    sub_filename = f\"submissions/{prep_name}_dnn_submission.csv\"\n",
    "    sub_df = pd.DataFrame({'id': test_ids, 'rainfall': test_preds})\n",
    "    sub_df.to_csv(sub_filename, index=False)\n",
    "    print(f\"Submission file saved: {sub_filename}\\n\")\n",
    "    \n",
    "    results.append({\n",
    "        'preprocessing': prep_name,\n",
    "        'best_auc': best_score,\n",
    "        'best_params': best_params,\n",
    "        'submission_file': sub_filename\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ea173e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Graphical Comparison of CV ROC AUC Scores\n",
    "\n",
    "We create a bar plot comparing the best CV ROC AUC scores for each preprocessing method..\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a1fc1",
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-11T13:46:00.135Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a bar plot to compare best CV ROC AUC scores\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='best_auc', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x='preprocessing', y='best_auc', data=results_df, palette='viridis')\n",
    "plt.title(\"Best CV ROC AUC by Preprocessing Method\")\n",
    "plt.xlabel(\"Preprocessing Method\")\n",
    "plt.ylabel(\"Best CV ROC AUC\")\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b9d177",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "- **Why Was the Score NaN?**  \n",
    "  The NaN ROC AUC score may occur if certain hyperparameter combinations cause unstable training (e.g., a too-high learning rate, improper network architecture, or other settings that cause the model’s loss to become NaN or its predictions to be constant). Expanding the hyperparameter search with additional layers and units gives the model more capacity and can help stabilize training.\n",
    "\n",
    "- **Expanded Hyperparameter Search:**  \n",
    "  We increased the search space to include the number of layers (2 or 3) and varied the number of neurons in each layer. Additionally, we expanded the ranges for dropout rates and learning rates and increased the number of iterations for RandomizedSearchCV.\n",
    "\n",
    "- **Graphical Comparison:**  \n",
    "  A bar plot shows the best CV ROC AUC scores for each preprocessing method, so you can visually inspect which one performed best.\n",
    "\n",
    "- **Final Submission:**  \n",
    "  The highest scoring submission is automatically copied to **submission.csv**, while all individual submissions are saved in the \"submissions\" folder.\n",
    "\n",
    "This modified and expanded search is designed to improve model performance in a competitive setting. Happy modeling and best of luck in the competition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392681dc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11251744,
     "sourceId": 91714,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36.443779,
   "end_time": "2025-03-18T11:31:21.170744",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-18T11:30:44.726965",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
