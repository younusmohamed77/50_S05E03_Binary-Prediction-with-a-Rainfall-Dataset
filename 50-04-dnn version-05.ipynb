{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "957a43b0",
   "metadata": {
    "papermill": {
     "duration": 0.004919,
     "end_time": "2025-03-10T15:17:32.787733",
     "exception": false,
     "start_time": "2025-03-10T15:17:32.782814",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cell 0: Check for GPU Availability\n",
    "\n",
    "Ensure that TensorFlow detects one or more GPUs. Run this cell in a GPU-enabled environment (e.g., Kaggle Kernels with GPU).\n",
    ".\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f03e0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T15:17:32.796936Z",
     "iopub.status.busy": "2025-03-10T15:17:32.796696Z",
     "iopub.status.idle": "2025-03-10T15:17:45.687751Z",
     "shell.execute_reply": "2025-03-10T15:17:45.686721Z"
    },
    "papermill": {
     "duration": 12.897125,
     "end_time": "2025-03-10T15:17:45.689374",
     "exception": false,
     "start_time": "2025-03-10T15:17:32.792249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7009c7dc",
   "metadata": {
    "papermill": {
     "duration": 0.003757,
     "end_time": "2025-03-10T15:17:45.697857",
     "exception": false,
     "start_time": "2025-03-10T15:17:45.694100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Libraries\n",
    "\n",
    "We import necessary libraries including scikeras for wrapping our Keras model, and also import SimpleImputer (for missing value imputation). GPU-enabled training will be used if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f03f810",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T15:17:45.706950Z",
     "iopub.status.busy": "2025-03-10T15:17:45.706482Z",
     "iopub.status.idle": "2025-03-10T15:17:53.729035Z",
     "shell.execute_reply": "2025-03-10T15:17:53.727953Z"
    },
    "papermill": {
     "duration": 8.029025,
     "end_time": "2025-03-10T15:17:53.730896",
     "exception": false,
     "start_time": "2025-03-10T15:17:45.701871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scikeras --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f23033b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T15:17:53.741281Z",
     "iopub.status.busy": "2025-03-10T15:17:53.740999Z",
     "iopub.status.idle": "2025-03-10T15:17:58.575489Z",
     "shell.execute_reply": "2025-03-10T15:17:58.574490Z"
    },
    "papermill": {
     "duration": 4.841449,
     "end_time": "2025-03-10T15:17:58.577359",
     "exception": false,
     "start_time": "2025-03-10T15:17:53.735910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, time\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Install scikeras if needed\n",
    "!pip install scikeras --quiet\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f95c621",
   "metadata": {
    "papermill": {
     "duration": 0.004249,
     "end_time": "2025-03-10T15:17:58.586217",
     "exception": false,
     "start_time": "2025-03-10T15:17:58.581968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Memory Optimization Function\r\n",
    "\r\n",
    "This function attempts to reduce memory usage by downcasting numeric columns to smaller data types (e.g., float64 -> float32, int64 -> int32.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4297dc10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T15:17:58.596926Z",
     "iopub.status.busy": "2025-03-10T15:17:58.595976Z",
     "iopub.status.idle": "2025-03-10T15:17:58.601793Z",
     "shell.execute_reply": "2025-03-10T15:17:58.601142Z"
    },
    "papermill": {
     "duration": 0.012654,
     "end_time": "2025-03-10T15:17:58.603261",
     "exception": false,
     "start_time": "2025-03-10T15:17:58.590607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define memory optimization function\n",
    "def reduce_memory_usage(df, verbose=True):\n",
    "    \"\"\"Downcasts numeric columns to reduce memory usage.\"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type == 'float64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        elif col_type == 'int64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f\"Memory usage reduced from {start_mem:.2f} MB to {end_mem:.2f} MB \"\n",
    "              f\"({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546772ee",
   "metadata": {
    "papermill": {
     "duration": 0.004254,
     "end_time": "2025-03-10T15:17:58.611921",
     "exception": false,
     "start_time": "2025-03-10T15:17:58.607667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load and Optimize Data\r\n",
    "\r\n",
    "We load our train/test data, apply the memory optimization function, and separate features/target. Adjust file names as needed for your environmen.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6278b76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T15:17:58.621432Z",
     "iopub.status.busy": "2025-03-10T15:17:58.621216Z",
     "iopub.status.idle": "2025-03-10T15:17:58.681540Z",
     "shell.execute_reply": "2025-03-10T15:17:58.680753Z"
    },
    "papermill": {
     "duration": 0.066394,
     "end_time": "2025-03-10T15:17:58.682774",
     "exception": false,
     "start_time": "2025-03-10T15:17:58.616380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before memory optimization:\n",
      "Train shape: (2190, 13)\n",
      "Test shape: (730, 12)\n",
      "Memory usage reduced from 0.22 MB to 0.09 MB (56.7% reduction)\n",
      "Memory usage reduced from 0.07 MB to 0.03 MB (54.1% reduction)\n",
      "After optimization:\n",
      "X shape: (2190, 11)\n",
      "X_test shape: (730, 11)\n"
     ]
    }
   ],
   "source": [
    "# Load and optimize data\n",
    "train = pd.read_csv('/kaggle/input/playground-series-s5e3/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/playground-series-s5e3/test.csv')\n",
    "\n",
    "print(\"Before memory optimization:\")\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "\n",
    "train = reduce_memory_usage(train)\n",
    "test = reduce_memory_usage(test)\n",
    "\n",
    "# Separate features and target from training data\n",
    "X = train.drop(columns=['id', 'rainfall'])\n",
    "y = train['rainfall']\n",
    "\n",
    "# For test data, drop the id column and save the IDs for submission\n",
    "X_test = test.drop(columns=['id'])\n",
    "test_ids = test['id']\n",
    "\n",
    "print(\"After optimization:\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b808fffb",
   "metadata": {
    "papermill": {
     "duration": 0.004012,
     "end_time": "2025-03-10T15:17:58.691242",
     "exception": false,
     "start_time": "2025-03-10T15:17:58.687230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Preprocessing Pipelines\r\n",
    "\r\n",
    "We create a dictionary of scalers to compare (none, standard, minmax, robust). If you prefer, you can remove or add more pipeline.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fe5d17e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T15:17:58.701023Z",
     "iopub.status.busy": "2025-03-10T15:17:58.700770Z",
     "iopub.status.idle": "2025-03-10T15:17:58.704300Z",
     "shell.execute_reply": "2025-03-10T15:17:58.703472Z"
    },
    "papermill": {
     "duration": 0.009614,
     "end_time": "2025-03-10T15:17:58.705572",
     "exception": false,
     "start_time": "2025-03-10T15:17:58.695958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define preprocessing pipelines\n",
    "preprocessors = {\n",
    "    'standard': StandardScaler(),\n",
    "    'minmax': MinMaxScaler(),\n",
    "    'robust': RobustScaler(),\n",
    "    'raw': None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9453e2",
   "metadata": {
    "papermill": {
     "duration": 0.004436,
     "end_time": "2025-03-10T15:17:58.714425",
     "exception": false,
     "start_time": "2025-03-10T15:17:58.709989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Keras Model Function\r\n",
    "\r\n",
    "We define a function that builds a DNN model. This will be used by the `KerasClassifier` wrapper. We'll allow hyperparameters like `optimizer`, `dropout_rate`, and `learning_rate` to be tune.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c072133",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T15:17:58.723602Z",
     "iopub.status.busy": "2025-03-10T15:17:58.723393Z",
     "iopub.status.idle": "2025-03-10T15:17:58.727860Z",
     "shell.execute_reply": "2025-03-10T15:17:58.727206Z"
    },
    "papermill": {
     "duration": 0.010367,
     "end_time": "2025-03-10T15:17:58.728986",
     "exception": false,
     "start_time": "2025-03-10T15:17:58.718619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the Keras model function\n",
    "def create_dnn_model(optimizer='adam', dropout_rate=0.2, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Build optimizer with the specified learning rate\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd90580",
   "metadata": {
    "papermill": {
     "duration": 0.004515,
     "end_time": "2025-03-10T15:17:58.738118",
     "exception": false,
     "start_time": "2025-03-10T15:17:58.733603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define KerasClassifier and Hyperparameter Grid\r\n",
    "\r\n",
    "We use the `KerasClassifier` wrapper from scikeras, which integrates with scikit-learn. We set up a parameter grid (or distribution) for random search. Adjust the ranges to fit your problem constraint\n",
    "We wrap our model with scikeras’ KerasClassifier. **Note:** For hyperparameters of the model-building function, we must use the prefix `model__` (and because the pipeline step is named `'dnn'`, we use `'dnn__model__'`)..\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43be593e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T15:17:58.747963Z",
     "iopub.status.busy": "2025-03-10T15:17:58.747737Z",
     "iopub.status.idle": "2025-03-10T15:17:58.751769Z",
     "shell.execute_reply": "2025-03-10T15:17:58.750892Z"
    },
    "papermill": {
     "duration": 0.01027,
     "end_time": "2025-03-10T15:17:58.753061",
     "exception": false,
     "start_time": "2025-03-10T15:17:58.742791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wrap the Keras model and define hyperparameter grid\n",
    "dnn_wrapper = KerasClassifier(\n",
    "    model=create_dnn_model,\n",
    "    epochs=20,           # Default epochs; will be tuned\n",
    "    batch_size=32,       # Default batch size; will be tuned\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Hyperparameter grid using correct prefix \"dnn__model__\" for parameters passed to create_dnn_model.\n",
    "param_dist = {\n",
    "    'dnn__model__optimizer': ['adam', 'rmsprop'],\n",
    "    'dnn__model__dropout_rate': [0.2, 0.3],\n",
    "    'dnn__model__learning_rate': [1e-3, 1e-4],\n",
    "    'dnn__epochs': [20, 30],\n",
    "    'dnn__batch_size': [32, 64]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8211f686",
   "metadata": {
    "papermill": {
     "duration": 0.004126,
     "end_time": "2025-03-10T15:17:58.761547",
     "exception": false,
     "start_time": "2025-03-10T15:17:58.757421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hyperparameter Tuning and Submission Generation (Robust Scaling Only)\n",
    "\n",
    "We build a pipeline that first imputes missing values (using median strategy), then applies RobustScaler (our only option), and finally trains the DNN. We run RandomizedSearchCV (using a threading backend) for thorough tuning, re‑fit the best estimator, and generate test predictions. A submission file is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75353aa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T15:17:58.771008Z",
     "iopub.status.busy": "2025-03-10T15:17:58.770761Z",
     "iopub.status.idle": "2025-03-10T15:27:05.261912Z",
     "shell.execute_reply": "2025-03-10T15:27:05.260879Z"
    },
    "papermill": {
     "duration": 546.497353,
     "end_time": "2025-03-10T15:27:05.263357",
     "exception": false,
     "start_time": "2025-03-10T15:17:58.766004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing: robust ---\n",
      "Best CV ROC AUC for robust: nan\n",
      "Best parameters: {'dnn__model__optimizer': 'adam', 'dnn__model__learning_rate': 0.001, 'dnn__model__dropout_rate': 0.2, 'dnn__epochs': 20, 'dnn__batch_size': 32}\n",
      "Submission file saved: submissions/robust_dnn_submission.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run hyperparameter tuning and generate submission for Robust scaling only\n",
    "from joblib import parallel_backend\n",
    "\n",
    "results = []  # List to store tuning results\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a directory for submission files\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "\n",
    "# Since we're only using Robust scaling:\n",
    "for prep_name, scaler in preprocessors.items():\n",
    "    if prep_name != 'robust':  # Skip other keys; we only want robust\n",
    "        continue\n",
    "    print(f\"\\n--- Preprocessing: {prep_name} ---\")\n",
    "    \n",
    "    # Build pipeline: imputer -> RobustScaler -> DNN\n",
    "    steps = []\n",
    "    steps.append(('imputer', SimpleImputer(strategy=\"median\")))\n",
    "    if scaler is not None:\n",
    "        steps.append((prep_name, scaler))\n",
    "    steps.append(('dnn', dnn_wrapper))\n",
    "    pipeline = Pipeline(steps)\n",
    "    \n",
    "    # Use threading backend to avoid pickling issues.\n",
    "    with parallel_backend('threading', n_jobs=-1):\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=50,\n",
    "            scoring='roc_auc',\n",
    "            cv=cv,\n",
    "            verbose=0,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        search.fit(X, y)\n",
    "    \n",
    "    best_score = search.best_score_\n",
    "    best_params = search.best_params_\n",
    "    \n",
    "    print(f\"Best CV ROC AUC for {prep_name}: {best_score:.4f}\")\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    \n",
    "    # Re-fit the best estimator on the full training data to ensure it is fitted.\n",
    "    best_pipe = search.best_estimator_\n",
    "    best_pipe.fit(X, y)\n",
    "    \n",
    "    # Instead of calling predict_proba on the entire pipeline (which causes __sklearn_tags__ error),\n",
    "    # manually transform X_test using all steps except the final one:\n",
    "    X_test_transformed = best_pipe[:-1].transform(X_test)\n",
    "    # Then call predict_proba on the DNN step:\n",
    "    test_preds = best_pipe.named_steps['dnn'].predict_proba(X_test_transformed)[:, 1]\n",
    "    \n",
    "    # Save the submission file for Robust scaling\n",
    "    sub_filename = f\"submissions/{prep_name}_dnn_submission.csv\"\n",
    "    sub_df = pd.DataFrame({'id': test_ids, 'rainfall': test_preds})\n",
    "    sub_df.to_csv(sub_filename, index=False)\n",
    "    print(f\"Submission file saved: {sub_filename}\\n\")\n",
    "    \n",
    "    # Record results for later comparison.\n",
    "    results.append({\n",
    "        'preprocessing': prep_name,\n",
    "        'best_auc': best_score,\n",
    "        'best_params': best_params,\n",
    "        'submission_file': sub_filename\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8e0006",
   "metadata": {
    "papermill": {
     "duration": 0.004559,
     "end_time": "2025-03-10T15:27:05.272799",
     "exception": false,
     "start_time": "2025-03-10T15:27:05.268240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Compare Results\r\n",
    "\r\n",
    "We create a DataFrame from the results and sort by the best AUC score to see which preprocessing pipeline + hyperparameters performed bes.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e71f2bfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T15:27:05.282466Z",
     "iopub.status.busy": "2025-03-10T15:27:05.282188Z",
     "iopub.status.idle": "2025-03-10T15:27:05.304516Z",
     "shell.execute_reply": "2025-03-10T15:27:05.303765Z"
    },
    "papermill": {
     "duration": 0.02849,
     "end_time": "2025-03-10T15:27:05.305720",
     "exception": false,
     "start_time": "2025-03-10T15:27:05.277230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Preprocessing Approaches:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>best_auc</th>\n",
       "      <th>best_params</th>\n",
       "      <th>submission_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robust</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'dnn__model__optimizer': 'adam', 'dnn__model_...</td>\n",
       "      <td>submissions/robust_dnn_submission.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  preprocessing  best_auc                                        best_params  \\\n",
       "0        robust       NaN  {'dnn__model__optimizer': 'adam', 'dnn__model_...   \n",
       "\n",
       "                         submission_file  \n",
       "0  submissions/robust_dnn_submission.csv  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare results across pipelines\n",
    "results_df = pd.DataFrame(results).sort_values(by='best_auc', ascending=False)\n",
    "print(\"\\nComparison of Preprocessing Approaches:\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c3eae",
   "metadata": {
    "papermill": {
     "duration": 0.004469,
     "end_time": "2025-03-10T15:27:05.314983",
     "exception": false,
     "start_time": "2025-03-10T15:27:05.310514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Copy Best Scoring Submission as submission.csv\n",
    "\n",
    "From the tuning results, we identify the best scoring submission and copy that file to \"submission.csv\" (the name required by the competition). All other submission file names remain unchanged..\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50f92b11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T15:27:05.324598Z",
     "iopub.status.busy": "2025-03-10T15:27:05.324381Z",
     "iopub.status.idle": "2025-03-10T15:27:05.694894Z",
     "shell.execute_reply": "2025-03-10T15:27:05.693653Z"
    },
    "papermill": {
     "duration": 0.376797,
     "end_time": "2025-03-10T15:27:05.696203",
     "exception": true,
     "start_time": "2025-03-10T15:27:05.319406",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No valid submission was found with a non-NaN ROC AUC score.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-54f747b73f02>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_auc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No valid submission was found with a non-NaN ROC AUC score.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mbest_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No valid submission was found with a non-NaN ROC AUC score."
     ]
    }
   ],
   "source": [
    "# Copy the best scoring submission to \"submission.csv\"\n",
    "import shutil\n",
    "\n",
    "# Filter out any candidates with NaN scores and pick the best candidate\n",
    "results_df = results_df[results_df['best_auc'].notna()]\n",
    "if len(results_df) == 0:\n",
    "    raise ValueError(\"No valid submission was found with a non-NaN ROC AUC score.\")\n",
    "\n",
    "best_result = results_df.iloc[0]\n",
    "best_submission_file = best_result['submission_file']\n",
    "\n",
    "# Copy the best submission file to \"submission.csv\"\n",
    "shutil.copy(best_submission_file, 'submission.csv')\n",
    "print(f\"Best scoring submission ({best_submission_file}) copied as 'submission.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2de13d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Retrain Final Models and Generate Final Submissions\n",
    "\n",
    "Optionally, you can retrain final models using the best hyperparameters from each pipeline on the entire training data and generate final submissions. This cell demonstrates that step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f0fe57",
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-08T21:24:41.694Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Retrain final models with best parameters and generate final submissions\n",
    "# for idx, row in results_df.iterrows():\n",
    "#     prep = row['preprocessing']\n",
    "#     best_params = row['best_params']\n",
    "#     print(f\"Retraining final model with preprocessing: {prep}\")\n",
    "    \n",
    "#     steps = []\n",
    "#     steps.append(('imputer', SimpleImputer(strategy=\"median\")))\n",
    "#     if prep in preprocessors and preprocessors[prep] is not None:\n",
    "#         steps.append((prep, preprocessors[prep]))\n",
    "#     steps.append(('dnn', dnn_wrapper.set_params(**best_params)))\n",
    "#     final_pipeline = Pipeline(steps)\n",
    "    \n",
    "#     final_pipeline.fit(X, y)\n",
    "#     final_preds = final_pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "#     final_sub_filename = f\"submissions/final_{prep}_dnn_submission.csv\"\n",
    "#     final_sub_df = pd.DataFrame({'id': test_ids, 'rainfall': final_preds})\n",
    "#     final_sub_df.to_csv(final_sub_filename, index=False)\n",
    "#     print(f\"Final submission saved: {final_sub_filename}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a57c19",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "- **Memory Optimization**: We downcast numeric columns to reduce memory usage.\n",
    "- **Missing Value Handling**: A SimpleImputer (median strategy) is included to ensure no NaN values persist.\n",
    "- **Preprocessing Pipelines**: We compared Standard, MinMax, Robust, and Raw (no scaling) in a pipeline with our DNN.\n",
    "- **GPU Acceleration**: The DNN is configured to run on GPU (if available) for competitive training speeds.\n",
    "- **Thorough Hyperparameter Tuning**: Using RandomizedSearchCV (50 iterations over 5-fold CV) we tuned optimizer, dropout rate, learning rate, epochs, and batch size.\n",
    "- **Submissions**: A submission file is generated for each preprocessing pipeline and saved in the \"submissions\" folder. The best scoring submission is explicitly copied to **submission.csv** as required.\n",
    "\n",
    "This code is designed for a competitive setting—adjust hyperparameter ranges, iterations, and preprocessing options as needed for further improvements. Happy modeling and best of luck in the competition!loration.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fdecf8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11251744,
     "sourceId": 91714,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 578.932018,
   "end_time": "2025-03-10T15:27:09.121352",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-10T15:17:30.189334",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
