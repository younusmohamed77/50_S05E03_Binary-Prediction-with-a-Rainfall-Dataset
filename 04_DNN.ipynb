{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d606f5-70e6-4812-9c8d-47cbf5c55feb",
   "metadata": {},
   "source": [
    "## DNN\n",
    "## Check for GPU Availability\r\n",
    "\r\n",
    "Ensure that TensorFlow detects a GPU. Run this cell in a GPU-enabled environment (e.g., Kaggle Kernels with GPU or a local GPU machine.\r\n",
    ".\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ba86191-508d-4ad6-aedb-de3836e45d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f106923-c593-4c20-86be-1c0266597be7",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "We import necessary libraries including scikeras for wrapping our Keras model, and also import SimpleImputer (for missing value imputation). GPU-enabled training will be used if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4ee12a-65cd-46aa-90ab-e72731d8b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, time\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# We'll use scikeras for wrapping Keras models\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aa6777-540e-4047-a176-62870f03df31",
   "metadata": {},
   "source": [
    "## Memory Optimization Function\r\n",
    "\r\n",
    "This function attempts to reduce memory usage by downcasting numeric columns to smaller data types (e.g., float64 -> float32, int64 -> int32.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65609a62-6dcf-4f22-b43c-1b389f11f102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define memory optimization function\n",
    "def reduce_memory_usage(df, verbose=True):\n",
    "    \"\"\"Downcasts numeric columns to reduce memory usage.\"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type == 'float64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        elif col_type == 'int64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f\"Memory usage reduced from {start_mem:.2f} MB to {end_mem:.2f} MB \"\n",
    "              f\"({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d8c3b-b819-4fc0-8885-667573748dcc",
   "metadata": {},
   "source": [
    "## Load and Optimize Data\r\n",
    "\r\n",
    "We load our train/test data, apply the memory optimization function, and separate features/target. Adjust file names as needed for your environmen.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b8b9a33-11d6-494b-a75f-0f9fc2e4f16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before memory optimization:\n",
      "Train shape: (2190, 13)\n",
      "Test shape: (730, 12)\n",
      "Memory usage reduced from 0.22 MB to 0.09 MB (56.7% reduction)\n",
      "Memory usage reduced from 0.07 MB to 0.03 MB (54.1% reduction)\n",
      "After optimization:\n",
      "X shape: (2190, 11)\n",
      "X_test shape: (730, 11)\n"
     ]
    }
   ],
   "source": [
    "# Load and optimize data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "print(\"Before memory optimization:\")\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "\n",
    "train = reduce_memory_usage(train)\n",
    "test = reduce_memory_usage(test)\n",
    "\n",
    "# Separate features and target from training data\n",
    "X = train.drop(columns=['id', 'rainfall'])\n",
    "y = train['rainfall']\n",
    "\n",
    "# For test data, drop the id column and save the IDs for submission\n",
    "X_test = test.drop(columns=['id'])\n",
    "test_ids = test['id']\n",
    "\n",
    "print(\"After optimization:\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8f456e-9000-4274-bc1d-4b71a6992bc8",
   "metadata": {},
   "source": [
    "## Define Preprocessing Pipelines\r\n",
    "\r\n",
    "We create a dictionary of scalers to compare (none, standard, minmax, robust). If you prefer, you can remove or add more pipeline.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77b4d389-0995-4830-a2aa-c1d19edcded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing pipelines\n",
    "preprocessors = {\n",
    "    'standard': StandardScaler(),\n",
    "    'minmax': MinMaxScaler(),\n",
    "    'robust': RobustScaler(),\n",
    "    'raw': None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c66c33-79cb-4111-ae39-0c6e2c84fa94",
   "metadata": {},
   "source": [
    "## Define Keras Model Function\r\n",
    "\r\n",
    "We define a function that builds a DNN model. This will be used by the `KerasClassifier` wrapper. We'll allow hyperparameters like `optimizer`, `dropout_rate`, and `learning_rate` to be tune.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9a44ae6-5edf-4dde-aafd-1e52bff83b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras model function\n",
    "def create_dnn_model(optimizer='adam', dropout_rate=0.2, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Build optimizer with the specified learning rate\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565398e0-7c88-4099-bcfc-d112b9ef73ac",
   "metadata": {},
   "source": [
    "## Define KerasClassifier and Hyperparameter Grid\r\n",
    "\r\n",
    "We use the `KerasClassifier` wrapper from scikeras, which integrates with scikit-learn. We set up a parameter grid (or distribution) for random search. Adjust the ranges to fit your problem constraint\n",
    "We wrap our model with scikerasâ€™ KerasClassifier. **Note:** For hyperparameters of the model-building function, we must use the prefix `model__` (and because the pipeline step is named `'dnn'`, we use `'dnn__model__'`)..\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "357aaab3-8bf0-4e0a-8c39-648c68ae2521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the KerasClassifier wrapper and hyperparameter grid\n",
    "dnn_wrapper = KerasClassifier(\n",
    "    model=create_dnn_model,\n",
    "    epochs=20,           # Default epochs; will be tuned further\n",
    "    batch_size=32,       # Default batch_size; will be tuned further\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid (using correct prefix \"dnn__model__\" for build_fn parameters)\n",
    "param_dist = {\n",
    "    'dnn__model__optimizer': ['adam', 'rmsprop'],\n",
    "    'dnn__model__dropout_rate': [0.2, 0.3],\n",
    "    'dnn__model__learning_rate': [1e-3, 1e-4],\n",
    "    'dnn__epochs': [20, 30],\n",
    "    'dnn__batch_size': [32, 64]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2401beba-dddc-49a8-8157-687cdff0bf8e",
   "metadata": {},
   "source": [
    "## Randomized Search for Each Preprocessing Pipeline\n",
    "\n",
    "We iterate through each scaler, create a pipeline that includes scaling + the DNN, and run `RandomizedSearchCV` to find the best hyperparameters. We collect the results in a list for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b50e65b6-2435-45c1-bd9d-8ef16d5edebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing: standard ---\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 32\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Perform RandomizedSearchCV with thorough tuning (50 iterations)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m     22\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mpipeline,\n\u001b[0;32m     23\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_dist,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     30\u001b[0m )\n\u001b[1;32m---> 32\u001b[0m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m best_score \u001b[38;5;241m=\u001b[39m search\u001b[38;5;241m.\u001b[39mbest_score_\n\u001b[0;32m     35\u001b[0m best_params \u001b[38;5;241m=\u001b[39m search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1809\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1808\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1809\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1811\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1812\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1813\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\joblib\\parallel.py:1703\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1703\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_abort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1706\u001b[0m     \u001b[38;5;66;03m# Store the unconsumed tasks and terminate the workers if necessary\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\joblib\\parallel.py:1614\u001b[0m, in \u001b[0;36mParallel._abort\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborted \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabort_everything\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m   1610\u001b[0m     \u001b[38;5;66;03m# If the backend is managed externally we need to make sure\u001b[39;00m\n\u001b[0;32m   1611\u001b[0m     \u001b[38;5;66;03m# to leave it in a working state to allow for future jobs\u001b[39;00m\n\u001b[0;32m   1612\u001b[0m     \u001b[38;5;66;03m# scheduling.\u001b[39;00m\n\u001b[0;32m   1613\u001b[0m     ensure_ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_managed_backend\n\u001b[1;32m-> 1614\u001b[0m     \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabort_everything\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensure_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ready\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\joblib\\_parallel_backends.py:620\u001b[0m, in \u001b[0;36mLokyBackend.abort_everything\u001b[1;34m(self, ensure_ready)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mabort_everything\u001b[39m(\u001b[38;5;28mself\u001b[39m, ensure_ready\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    618\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Shutdown the workers and restart a new one with the same parameters\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkill_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ensure_ready:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\joblib\\executor.py:75\u001b[0m, in \u001b[0;36mMemmappingExecutor.terminate\u001b[1;34m(self, kill_workers)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mterminate\u001b[39m(\u001b[38;5;28mself\u001b[39m, kill_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkill_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkill_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# When workers are killed in a brutal manner, they cannot execute the\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# finalizer of their shared memmaps. The refcount of those memmaps may\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# be off by an unknown number, so instead of decref'ing them, we force\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# with allow_non_empty=True but if we can't, it will be clean up later\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# on by the resource_tracker.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_submit_resize_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:1303\u001b[0m, in \u001b[0;36mProcessPoolExecutor.shutdown\u001b[1;34m(self, wait, kill_workers)\u001b[0m\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executor_manager_thread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m wait:\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;66;03m# This locks avoids concurrent join if the interpreter\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m     \u001b[38;5;66;03m# is shutting down.\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _global_shutdown_lock:\n\u001b[1;32m-> 1303\u001b[0m         \u001b[43mexecutor_manager_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1304\u001b[0m         _threads_wakeups\u001b[38;5;241m.\u001b[39mpop(executor_manager_thread, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# To reduce the risk of opening too many files, remove references to\u001b[39;00m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;66;03m# objects that use file descriptors.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\threading.py:1011\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1008\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1011\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1013\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\threading.py:1027\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# already determined that the C code is done\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[1;32m-> 1027\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1028\u001b[0m     lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run hyperparameter tuning and generate submissions\n",
    "results = []  # List to store results for later comparison\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a directory for submission files\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "\n",
    "for prep_name, scaler in preprocessors.items():\n",
    "    print(f\"\\n--- Preprocessing: {prep_name} ---\")\n",
    "    \n",
    "    # Build the pipeline: always include an imputer to handle missing values,\n",
    "    # then add the scaler if specified, and finally the DNN.\n",
    "    steps = []\n",
    "    steps.append(('imputer', SimpleImputer(strategy=\"median\")))\n",
    "    if scaler is not None:\n",
    "        steps.append((prep_name, scaler))\n",
    "    steps.append(('dnn', dnn_wrapper))\n",
    "    pipeline = Pipeline(steps)\n",
    "    \n",
    "    # Perform RandomizedSearchCV with thorough tuning (50 iterations)\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=50,\n",
    "        scoring='roc_auc',\n",
    "        cv=cv,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    search.fit(X, y)\n",
    "    \n",
    "    best_score = search.best_score_\n",
    "    best_params = search.best_params_\n",
    "    \n",
    "    print(f\"Best CV ROC AUC for {prep_name}: {best_score:.4f}\")\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    \n",
    "    # Generate test predictions using the best estimator from the pipeline.\n",
    "    # The imputer and scaler steps will automatically transform X_test.\n",
    "    test_preds = search.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Save the submission file for this preprocessing method.\n",
    "    sub_filename = f\"submissions/{prep_name}_dnn_submission.csv\"\n",
    "    sub_df = pd.DataFrame({'id': test_ids, 'rainfall': test_preds})\n",
    "    sub_df.to_csv(sub_filename, index=False)\n",
    "    print(f\"Submission file saved: {sub_filename}\\n\")\n",
    "    \n",
    "    # Record results for later comparison.\n",
    "    results.append({\n",
    "        'preprocessing': prep_name,\n",
    "        'best_auc': best_score,\n",
    "        'best_params': best_params,\n",
    "        'submission_file': sub_filename\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e11b3-4b28-44ab-95c3-0d638db8af72",
   "metadata": {},
   "source": [
    "## Compare Results\r\n",
    "\r\n",
    "We create a DataFrame from the results and sort by the best AUC score to see which preprocessing pipeline + hyperparameters performed bes.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc8694-7f2c-4d39-b6c4-8319f93aa603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results across different preprocessing pipelines\n",
    "results_df = pd.DataFrame(results).sort_values(by='best_auc', ascending=False)\n",
    "print(\"\\nComparison of Preprocessing Approaches:\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86305d8-6ff8-4464-ae57-a837a3760a4f",
   "metadata": {},
   "source": [
    "## Train Final Model & Generate Predictions\r\n",
    "\r\n",
    "You may choose to train a final model using the best pipeline & parameters, then generate predictions for your test set. Below is an example of how to do this with the top result from `results_df.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcbc13d-121b-4709-8b98-92af28850b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain final models using best parameters and generate submission files\n",
    "for idx, row in results_df.iterrows():\n",
    "    prep = row['preprocessing']\n",
    "    best_params = row['best_params']\n",
    "    print(f\"Retraining final model with preprocessing: {prep}\")\n",
    "    \n",
    "    if prep in preprocessors and preprocessors[prep] is not None:\n",
    "        final_pipeline = Pipeline([\n",
    "            (prep, preprocessors[prep]),\n",
    "            ('dnn', dnn_wrapper.set_params(**best_params))\n",
    "        ])\n",
    "    else:\n",
    "        final_pipeline = Pipeline([\n",
    "            ('dnn', dnn_wrapper.set_params(**best_params))\n",
    "        ])\n",
    "    \n",
    "    final_pipeline.fit(X, y)\n",
    "    final_preds = final_pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    final_sub_filename = f\"submissions/final_{prep}_dnn_submission.csv\"\n",
    "    final_sub_df = pd.DataFrame({'id': test_ids, 'rainfall': final_preds})\n",
    "    final_sub_df.to_csv(final_sub_filename, index=False)\n",
    "    print(f\"Final submission saved: {final_sub_filename}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f5bbd-8abb-4527-9918-c1cf8b014915",
   "metadata": {},
   "source": [
    "# Conclusion\r\n",
    "\r\n",
    "1. **Memory Optimization**: We used a simple function to downcast numerical columns, reducing the dataset's memory footprint.\r\n",
    "2. **Preprocessing Comparison**: We compared several scalers (Standard, MinMax, Robust) in a pipeline with a DNN.\r\n",
    "3. **Hyperparameter Tuning**: Used `RandomizedSearchCV` to explore different dropout rates and learning rates.\r\n",
    "4. **Results**: We displayed each pipelineâ€™s best ROC AUC score and hyperparameters, then retrained a final model for submission.\r\n",
    "\r\n",
    "This approach can be extended by:\r\n",
    "- Adding more preprocessing techniques (e.g., PCA, custom transformations).\r\n",
    "- Tuning additional hyperparameters (batch size, epochs, network architecture).\r\n",
    "- Using more folds or a larger `n_iter` for RandomizedSearchCV for better hyperparameter exloration.\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
