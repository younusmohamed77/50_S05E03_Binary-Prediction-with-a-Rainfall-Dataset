{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc84d37b-6e61-4548-9ef5-8cf1d010f67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Import libraries for modeling ---\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "# --- Load datasets ---\n",
    "df_train = pd.read_csv('/kaggle/input/playground-series-s5e3/train.csv')\n",
    "df_test  = pd.read_csv('/kaggle/input/playground-series-s5e3/test.csv')\n",
    "df_sub = pd.read_csv('/kaggle/input/playground-series-s5e3/sample_submission.csv')\n",
    "\n",
    "# Drop identifier column\n",
    "df_train.drop(columns=['id'], inplace=True)\n",
    "df_test.drop(columns=['id'], inplace=True)\n",
    "\n",
    "# Fill missing values in test data (if any)\n",
    "if df_test['winddirection'].isnull().sum() > 0:\n",
    "    df_test['winddirection'].fillna(df_test['winddirection'].median(), inplace=True)\n",
    "\n",
    "# --- Merge External Data (if applicable) ---\n",
    "df_ext = pd.read_csv('/kaggle/input/rainfall-prediction-using-machine-learning/Rainfall.csv')\n",
    "df_ext = df_ext.rename(columns={'pressure ': 'pressure', \n",
    "                                'humidity ': 'humidity', \n",
    "                                'cloud ': 'cloud',\n",
    "                                'winddirection': 'winddirection'})\n",
    "df_train = pd.concat([df_train, df_ext], axis=0, ignore_index=True)\n",
    "df_train = df_train.replace({'yes': 1, 'no': 0})\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "def create_weather_features(df):    \n",
    "    df[\"temp_Range\"] = df[\"maxtemp\"] - df[\"mintemp\"]\n",
    "    df[\"Heat_Index\"] = df[\"temparature\"] + 0.5 * (df[\"temparature\"] - 10) * (df[\"humidity\"] / 100)    \n",
    "    df['csr'] = df['cloud'] / (df['sunshine'] + 1e-5)\n",
    "    df[\"Dew_Dep\"] = df[\"temparature\"] - df[\"dewpoint\"]    \n",
    "    df[\"Wind_Chill\"] = np.where(\n",
    "        df[\"temparature\"] < 10,\n",
    "        13.12 + 0.6215 * df[\"temparature\"] - 11.37 * df[\"windspeed\"]**0.16 + \n",
    "        0.3965 * df[\"temparature\"] * df[\"windspeed\"]**0.16,\n",
    "        df[\"temparature\"]\n",
    "    )   \n",
    "    df['hsi'] = df['humidity'] * df['sunshine']\n",
    "    df[\"Cloud_Sun_Ratio\"] = df[\"cloud\"] / (df[\"sunshine\"] + 1)    \n",
    "    df[\"Pressure_Change\"] = df[\"pressure\"].diff().fillna(0)    \n",
    "    air_density = 1.225  \n",
    "    df['wi'] = (0.4 * df['humidity']) + (0.3 * df['cloud']) - (0.3 * df['sunshine'])\n",
    "    df[\"Wind_Power\"] = 0.5 * air_density * df[\"windspeed\"]**3         \n",
    "    df['cloud + humidity'] = df['cloud'] + df['humidity']\n",
    "    df['cloud + humidity + sunshine'] = df['cloud'] + df['humidity'] + df['sunshine']\n",
    "    df['sp'] = df['sunshine'] / (df['sunshine'] + df['cloud'] + 1e-5)\n",
    "    df['cloud * sunshine'] = df['cloud'] * df['sunshine']\n",
    "    df['humidity * sunshine'] = df['humidity'] * df['sunshine']\n",
    "    df['rd'] = 100 - df['humidity']\n",
    "    df[\"HTI\"] = df[\"temparature\"] + 0.2 * df[\"humidity\"]    \n",
    "    df[\"ACI\"] = (df[\"maxtemp\"] + df[\"mintemp\"]) / 2 - (df[\"humidity\"] / 2) + df[\"dewpoint\"]    \n",
    "    df[\"CSI\"] = (df[\"sunshine\"] - df[\"cloud\"]) / (df[\"sunshine\"] + df[\"cloud\"] + 1)\n",
    "    df['hci'] = df['humidity'] * df['cloud']\n",
    "    df[\"WCI\"] = df[\"temparature\"] - 0.5 * df[\"windspeed\"]\n",
    "    # Convert day column to datetime; later it is dropped.\n",
    "    df['day'] = pd.to_datetime(df['day'], errors='coerce')    \n",
    "    return df\n",
    "\n",
    "df_train = create_weather_features(df_train)\n",
    "df_test = create_weather_features(df_test)\n",
    "\n",
    "# Fill missing values using the median (only numeric columns)\n",
    "df_train.fillna(df_train.median(numeric_only=True), inplace=True)\n",
    "df_test.fillna(df_test.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Drop unneeded columns ('day' and any extra misnamed columns)\n",
    "if 'day' in df_train.columns:\n",
    "    df_train.drop(columns=['day'], inplace=True)\n",
    "if '         winddirection' in df_train.columns:\n",
    "    df_train.drop(columns=['         winddirection'], inplace=True)\n",
    "if 'day' in df_test.columns:\n",
    "    df_test.drop(columns=['day'], inplace=True)\n",
    "\n",
    "# Separate target and features\n",
    "y = df_train['rainfall']\n",
    "df_train.drop(columns=['rainfall'], inplace=True)\n",
    "X = df_train.copy()\n",
    "\n",
    "# --- Deep Learning Model (Conv1D) ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(df_test)\n",
    "\n",
    "# Split data for deep learning\n",
    "X_train_dl, X_val_dl, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# Reshape for Conv1D: (samples, features, 1)\n",
    "X_train_dl = X_train_dl.reshape((X_train_dl.shape[0], X_train_dl.shape[1], 1))\n",
    "X_val_dl = X_val_dl.reshape((X_val_dl.shape[0], X_val_dl.shape[1], 1))\n",
    "X_test_dl = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "# Build the Conv1D model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_dl.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[AUC(name='auc')])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-5, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_dl, y_train,\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_dl, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Get predictions from the deep learning model on test set\n",
    "test_preds_dl = model.predict(X_test_dl).flatten()\n",
    "\n",
    "# --- LightGBM Model using StratifiedKFold ---\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Arrays to store out-of-fold predictions and test predictions\n",
    "lgbm_oof_preds = np.zeros(len(X))\n",
    "lgbm_test_preds = np.zeros(len(df_test))\n",
    "\n",
    "lgb_params = {\n",
    "    'max_depth': 11, \n",
    "    'num_leaves': 152, \n",
    "    'learning_rate': 0.1284552398987031,\n",
    "    'feature_fraction': 0.699205159877181965, \n",
    "    'bagging_fraction': 0.964565149953761379, \n",
    "    'bagging_freq': 3, \n",
    "    'min_child_samples': 93, \n",
    "    'lambda_l1': 1.4313376954414664e-09, \n",
    "    'lambda_l2': 5.9890062038048195, \n",
    "    'min_gain_to_split': 0.5655566115584013\n",
    "}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train_lgb = X.iloc[train_idx]\n",
    "    X_val_lgb = X.iloc[val_idx]\n",
    "    y_train_lgb = y.iloc[train_idx]\n",
    "    y_val_lgb = y.iloc[val_idx]\n",
    "    \n",
    "    lgbm_model = LGBMClassifier(**lgb_params)\n",
    "    lgbm_model.fit(X_train_lgb, y_train_lgb,\n",
    "                   eval_set=[(X_val_lgb, y_val_lgb)],\n",
    "                   eval_metric='auc',\n",
    "                   verbose=False)\n",
    "    \n",
    "    lgbm_oof_preds[val_idx] = lgbm_model.predict_proba(X_val_lgb)[:, 1]\n",
    "    lgbm_test_preds += lgbm_model.predict_proba(df_test)[:, 1] / n_splits\n",
    "\n",
    "overall_auc_lgbm = roc_auc_score(y, lgbm_oof_preds)\n",
    "print(\"Overall AUC (LightGBM):\", overall_auc_lgbm)\n",
    "\n",
    "# --- CatBoost Model using StratifiedKFold ---\n",
    "# First, label encode any categorical features (if they exist)\n",
    "categorical_columns = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "cat_features = [X.columns.get_loc(col) for col in categorical_columns]\n",
    "\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "skf_cat = StratifiedKFold(n_splits=5, shuffle=True, random_state=41)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf_cat.split(X, y), 1):\n",
    "    X_train_cat = X.iloc[train_idx]\n",
    "    X_val_cat = X.iloc[val_idx]\n",
    "    y_train_cat = y.iloc[train_idx]\n",
    "    y_val_cat = y.iloc[val_idx]\n",
    "    \n",
    "    cat_model = CatBoostClassifier(\n",
    "        iterations=1000,\n",
    "        learning_rate=0.1,\n",
    "        depth=6,\n",
    "        cat_features=cat_features,\n",
    "        verbose=200,\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    \n",
    "    cat_model.fit(X_train_cat, y_train_cat, eval_set=(X_val_cat, y_val_cat),\n",
    "                  early_stopping_rounds=10, verbose=200)\n",
    "    \n",
    "    y_pred_cat = cat_model.predict(X_val_cat)\n",
    "    y_pred_proba_cat = cat_model.predict_proba(X_val_cat)[:, 1]\n",
    "    acc = accuracy_score(y_val_cat, y_pred_cat)\n",
    "    f1 = f1_score(y_val_cat, y_pred_cat, average=\"weighted\")\n",
    "    roc_auc = roc_auc_score(y_val_cat, y_pred_proba_cat)\n",
    "    acc_list.append(acc)\n",
    "    f1_list.append(f1)\n",
    "    roc_auc_list.append(roc_auc)\n",
    "    print(f\"CatBoost Fold {fold} - Accuracy: {acc:.4f}, F1-Score: {f1:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "print(\"CatBoost CV - Mean Accuracy:\", np.mean(acc_list))\n",
    "print(\"CatBoost CV - Mean F1-Score:\", np.mean(f1_list))\n",
    "print(\"CatBoost CV - Mean ROC-AUC:\", np.mean(roc_auc_list))\n",
    "\n",
    "cat_test_preds = cat_model.predict_proba(df_test)[:, 1]\n",
    "\n",
    "# --- Final Ensemble and Submission ---\n",
    "# Combine predictions from LightGBM, CatBoost, and the Deep Learning model.\n",
    "# (Weights here are arbitrary and can be tuned)\n",
    "ensemble_preds = (lgbm_test_preds + cat_test_preds) * 0.5 + test_preds_dl\n",
    "\n",
    "# Ensure final predictions are in the range [0, 1]\n",
    "ensemble_preds = np.clip(ensemble_preds, 0, 1)\n",
    "\n",
    "df_sub['rainfall'] = ensemble_preds\n",
    "df_sub.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file saved as 'submission.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446250f0-02e6-4b94-92d5-f50d4192a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. IMPORTS & SETUP\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import optuna\n",
    "import time\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. DATA LOADING & INITIAL PROCESSING\n",
    "# ============================================================\n",
    "\n",
    "# Read competition data and an external \"original\" dataset; fix misspellings.\n",
    "train = pd.read_csv('/kaggle/input/playground-series-s5e3/train.csv').rename(columns={'temparature': 'temperature'})\n",
    "test = pd.read_csv('/kaggle/input/playground-series-s5e3/test.csv').rename(columns={'temparature': 'temperature'})\n",
    "original = pd.read_csv(\"/kaggle/input/rainfall-prediction-using-machine-learning/Rainfall.csv\").rename(columns={'temparature': 'temperature'})\n",
    "submission = pd.read_csv(\"/kaggle/input/playground-series-s5e3/sample_submission.csv\")\n",
    "\n",
    "# Drop identifier columns\n",
    "if 'id' in train.columns:\n",
    "    train.drop(columns=['id'], inplace=True)\n",
    "if 'id' in test.columns:\n",
    "    test.drop(columns=['id'], inplace=True)\n",
    "\n",
    "# Map external target values from strings to binary numbers and strip whitespace from column names.\n",
    "original['rainfall'] = original['rainfall'].map({'yes': 1, 'no': 0})\n",
    "original.columns = [col.strip() for col in original.columns]\n",
    "\n",
    "# Mark the data source\n",
    "train[\"original\"] = 0\n",
    "test[\"original\"] = 0\n",
    "original[\"original\"] = 1\n",
    "\n",
    "# Concatenate external data to training data\n",
    "train = pd.concat([train, original], axis=0).reset_index(drop=True)\n",
    "target = 'rainfall'\n",
    "\n",
    "# ============================================================\n",
    "# 3. PREPROCESSING FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def fix_day_sequence_with_month_year(df, day_column='day', cycle_length=365):\n",
    "    \"\"\"\n",
    "    Corrects the 'day' column sequence and creates 'month' and 'year' columns.\n",
    "    \"\"\"\n",
    "    df_fixed = df.copy()\n",
    "    days = df_fixed[day_column].values\n",
    "    fixed_days = np.zeros_like(days)\n",
    "    months = np.zeros_like(days)\n",
    "    years = np.ones_like(days)  # start with year 1\n",
    "    \n",
    "    # Days per month for a non-leap year\n",
    "    days_per_month = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "    \n",
    "    current_day = 1\n",
    "    current_month = 1\n",
    "    current_year = 1\n",
    "    \n",
    "    fixed_days[0] = current_day\n",
    "    months[0] = current_month\n",
    "    years[0] = current_year\n",
    "    \n",
    "    for i in range(1, len(days)):\n",
    "        expected_next = current_day + 1 if current_day < cycle_length else 1\n",
    "        current_value = days[i]\n",
    "        if (current_value == expected_next) or (abs(current_value - expected_next) <= 5 and current_value <= cycle_length) or (current_day == cycle_length and current_value == 1):\n",
    "            current_day = current_value\n",
    "        else:\n",
    "            current_day = expected_next\n",
    "        \n",
    "        if current_day == 1 and i > 0:\n",
    "            current_year += 1\n",
    "        \n",
    "        day_of_year = current_day if current_day != 1 else 1\n",
    "        cumulative = 0\n",
    "        for month, days_in_month in enumerate(days_per_month, 1):\n",
    "            if day_of_year <= cumulative + days_in_month:\n",
    "                current_month = month\n",
    "                break\n",
    "            cumulative += days_in_month\n",
    "        \n",
    "        fixed_days[i] = current_day\n",
    "        months[i] = current_month\n",
    "        years[i] = current_year\n",
    "        \n",
    "    df_fixed[day_column] = fixed_days\n",
    "    df_fixed['month'] = months\n",
    "    df_fixed['year'] = years\n",
    "    return df_fixed\n",
    "\n",
    "def handle_missing_values(train_df, test_df, target=\"rainfall\", n_components=1):\n",
    "    \"\"\"\n",
    "    Imputes missing numeric values using the median (fitted on train) and adds indicator columns for missingness.\n",
    "    If multiple indicators exist, applies Truncated SVD to reduce dimensionality.\n",
    "    \"\"\"\n",
    "    # Use only common features (excluding the target)\n",
    "    common_features = [col for col in train_df.columns if col in test_df.columns and col != target]\n",
    "    train_subset = train_df[common_features]\n",
    "    test_subset = test_df[common_features]\n",
    "    \n",
    "    imp = SimpleImputer(strategy='median')\n",
    "    imp.fit(train_subset)\n",
    "    train_imputed_values = imp.transform(train_subset)\n",
    "    test_imputed_values = imp.transform(test_subset)\n",
    "    \n",
    "    train_imputed = pd.DataFrame(train_imputed_values, columns=common_features, index=train_df.index)\n",
    "    test_imputed = pd.DataFrame(test_imputed_values, columns=common_features, index=test_df.index)\n",
    "    \n",
    "    # Bring back non-common features from train (if any)\n",
    "    non_common = [col for col in train_df.columns if col not in common_features and col != target]\n",
    "    for col in non_common:\n",
    "        train_imputed[col] = train_df[col]\n",
    "    \n",
    "    # Create missing indicators\n",
    "    indicator_cols = []\n",
    "    for col in common_features:\n",
    "        indicator = f'{col}_missing'\n",
    "        train_imputed[indicator] = train_df[col].isna().astype(int)\n",
    "        test_imputed[indicator] = test_df[col].isna().astype(int)\n",
    "        indicator_cols.append(indicator)\n",
    "    \n",
    "    # Optionally reduce indicator dimensions using SVD if more than one exists\n",
    "    if len(indicator_cols) > 1:\n",
    "        from sklearn.decomposition import TruncatedSVD\n",
    "        svd = TruncatedSVD(n_components=min(n_components, len(indicator_cols)))\n",
    "        indicators_train = train_imputed[indicator_cols].values\n",
    "        indicators_test = test_imputed[indicator_cols].values\n",
    "        if np.any(indicators_train):\n",
    "            svd_train = svd.fit_transform(indicators_train)\n",
    "            svd_test = svd.transform(indicators_test)\n",
    "            for i in range(n_components):\n",
    "                train_imputed[f'missing_svd_{i}'] = svd_train[:, i]\n",
    "                test_imputed[f'missing_svd_{i}'] = svd_test[:, i]\n",
    "            train_imputed.drop(columns=indicator_cols, inplace=True)\n",
    "            test_imputed.drop(columns=indicator_cols, inplace=True)\n",
    "    \n",
    "    # Add back target to training data\n",
    "    if target in train_df.columns:\n",
    "        train_imputed[target] = train_df[target]\n",
    "    return train_imputed, test_imputed\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Creates derived features based on meteorological domain knowledge.\n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    # 1. Temperature range\n",
    "    df_new['temp_range'] = df_new['maxtemp'] - df_new['mintemp']\n",
    "    # 2. Dewpoint depression\n",
    "    df_new['dewpoint_depression'] = df_new['temperature'] - df_new['dewpoint']\n",
    "    # 3. Pressure change (first difference)\n",
    "    df_new['pressure_change'] = df_new['pressure'].diff().fillna(0)\n",
    "    # 4. Humidity-to-dewpoint ratio\n",
    "    df_new['humidity_dewpoint_ratio'] = df_new['humidity'] / df_new['dewpoint'].clip(lower=0.1)\n",
    "    # 5. Cloud-to-sunshine ratio\n",
    "    df_new['cloud_sunshine_ratio'] = df_new['cloud'] / df_new['sunshine'].clip(lower=0.1)\n",
    "    # 6. Wind-humidity factor\n",
    "    df_new['wind_humidity_factor'] = df_new['windspeed'] * (df_new['humidity'] / 100)\n",
    "    # 7. Temperature-humidity index\n",
    "    df_new['temp_humidity_index'] = (0.8 * df_new['temperature']) + ((df_new['humidity'] / 100) * (df_new['temperature'] - 14.3)) + 46.4\n",
    "    # 8. Pressure acceleration (second difference)\n",
    "    df_new['pressure_acceleration'] = df_new['pressure_change'].diff().fillna(0)\n",
    "    # 9. Cyclical transformation for day-of-year\n",
    "    df_new['day_of_year_sin'] = np.sin(2 * np.pi * df_new['day'] / 365)\n",
    "    df_new['day_of_year_cos'] = np.cos(2 * np.pi * df_new['day'] / 365)\n",
    "    # 10. Rolling averages (3, 7, 14-day windows)\n",
    "    for window in [3, 7, 14]:\n",
    "        df_new[f'temperature_rolling_{window}d'] = df_new['temperature'].rolling(window=window, min_periods=1).mean()\n",
    "        df_new[f'pressure_rolling_{window}d'] = df_new['pressure'].rolling(window=window, min_periods=1).mean()\n",
    "        df_new[f'humidity_rolling_{window}d'] = df_new['humidity'].rolling(window=window, min_periods=1).mean()\n",
    "        df_new[f'cloud_rolling_{window}d'] = df_new['cloud'].rolling(window=window, min_periods=1).mean()\n",
    "        df_new[f'windspeed_rolling_{window}d'] = df_new['windspeed'].rolling(window=window, min_periods=1).mean()\n",
    "    # 11. 3-day trend features\n",
    "    df_new['temp_trend_3d'] = df_new['temperature'].diff(3).fillna(0)\n",
    "    df_new['pressure_trend_3d'] = df_new['pressure'].diff(3).fillna(0)\n",
    "    df_new['humidity_trend_3d'] = df_new['humidity'].diff(3).fillna(0)\n",
    "    # 12. Extreme weather indicators\n",
    "    df_new['extreme_temp'] = ((df_new['temperature'] > df_new['temperature'].quantile(0.95)) |\n",
    "                              (df_new['temperature'] < df_new['temperature'].quantile(0.05))).astype(int)\n",
    "    df_new['extreme_humidity'] = ((df_new['humidity'] > df_new['humidity'].quantile(0.95)) |\n",
    "                                  (df_new['humidity'] < df_new['humidity'].quantile(0.05))).astype(int)\n",
    "    df_new['extreme_pressure'] = ((df_new['pressure'] > df_new['pressure'].quantile(0.95)) |\n",
    "                                  (df_new['pressure'] < df_new['pressure'].quantile(0.05))).astype(int)\n",
    "    # 13. Interaction features\n",
    "    df_new['temp_humidity_interaction'] = df_new['temperature'] * df_new['humidity']\n",
    "    df_new['pressure_wind_interaction'] = df_new['pressure'] * df_new['windspeed']\n",
    "    df_new['cloud_sunshine_interaction'] = df_new['cloud'] * df_new['sunshine']\n",
    "    df_new['dewpoint_humidity_interaction'] = df_new['dewpoint'] * df_new['humidity']\n",
    "    # 14. Rolling standard deviations (7 & 14 days)\n",
    "    for window in [7, 14]:\n",
    "        df_new[f'temp_std_{window}d'] = df_new['temperature'].rolling(window=window, min_periods=4).std().fillna(0)\n",
    "        df_new[f'pressure_std_{window}d'] = df_new['pressure'].rolling(window=window, min_periods=4).std().fillna(0)\n",
    "        df_new[f'humidity_std_{window}d'] = df_new['humidity'].rolling(window=window, min_periods=4).std().fillna(0)\n",
    "    # 15. Seasonal feature (using already computed 'month')\n",
    "    df_new['season'] = ((df_new['month'] - 1) // 3) + 1\n",
    "    return df_new\n",
    "\n",
    "# ============================================================\n",
    "# 4. APPLY PREPROCESSING & FEATURE ENGINEERING\n",
    "# ============================================================\n",
    "\n",
    "# Fix day sequence (adds month and year)\n",
    "train = fix_day_sequence_with_month_year(train)\n",
    "test = fix_day_sequence_with_month_year(test)\n",
    "\n",
    "# Impute missing values and add missing indicators\n",
    "train_imputed, test_imputed = handle_missing_values(train, test, target=target, n_components=1)\n",
    "\n",
    "# (Optional: One Hot Encode categorical features if needed using OHE function)\n",
    "\n",
    "# Create new derived features\n",
    "train_fe = engineer_features(train_imputed)\n",
    "test_fe = engineer_features(test_imputed)\n",
    "\n",
    "# ============================================================\n",
    "# 5. FEATURE SELECTION & SCALING\n",
    "# ============================================================\n",
    "\n",
    "# Separate target; get all features except target\n",
    "final_features = [f for f in train_fe.columns if f != target]\n",
    "final_features = list(set(final_features))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_scaled = train_fe.copy()\n",
    "test_scaled = test_fe.copy()\n",
    "train_scaled[final_features] = scaler.fit_transform(train_fe[final_features])\n",
    "test_scaled[final_features] = scaler.transform(test_fe[final_features])\n",
    "\n",
    "# Remove duplicate (redundant) features\n",
    "def post_processor(train_df, test_df, target):\n",
    "    cols = train_df.drop(columns=[target]).columns\n",
    "    train_copy = train_df.copy()\n",
    "    test_copy = test_df.copy()\n",
    "    drop_cols = []\n",
    "    for i, feature in enumerate(cols):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            if np.sum(np.abs(train_copy[feature] - train_copy[cols[j]])) == 0:\n",
    "                if cols[j] not in drop_cols:\n",
    "                    drop_cols.append(cols[j])\n",
    "    train_copy.drop(columns=drop_cols, inplace=True)\n",
    "    test_copy.drop(columns=drop_cols, inplace=True)\n",
    "    return train_copy, test_copy\n",
    "\n",
    "train_cop, test_cop = post_processor(train_scaled, test_scaled, target)\n",
    "X_train = train_cop.drop(columns=[target])\n",
    "y_train = train_cop[target]\n",
    "X_test = test_cop.copy()\n",
    "\n",
    "# (Optional: Use a feature importance method to select top features)\n",
    "def get_most_important_features(X_train, y_train, n, model_input):\n",
    "    # Here we use XGBoost as an example\n",
    "    xgb_params = {\n",
    "        'n_jobs': -1,\n",
    "        'eval_metric': 'logloss',\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'hist',\n",
    "        'verbosity': 0,\n",
    "        'random_state': 42,\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**xgb_params)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    auc_scores = []\n",
    "    importances = []\n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        model.fit(X_tr, y_tr, verbose=False)\n",
    "        y_pred = model.predict_proba(X_val)[:, 1]\n",
    "        auc_scores.append(roc_auc_score(y_val, y_pred))\n",
    "        importances.append(model.feature_importances_)\n",
    "    avg_importance = np.mean(importances, axis=0)\n",
    "    features = [(X_train.columns[i], avg_importance[i]) for i in range(len(X_train.columns))]\n",
    "    sorted_features = sorted(features, key=lambda x: x[1], reverse=True)\n",
    "    top_n = [f[0] for f in sorted_features[:n]]\n",
    "    return top_n\n",
    "\n",
    "# For example, select top 50 features from XGBoost importance:\n",
    "top_features = get_most_important_features(X_train.reset_index(drop=True), y_train, 50, 'xgb')\n",
    "X_train = X_train[top_features]\n",
    "X_test = X_test[top_features]\n",
    "\n",
    "# ============================================================\n",
    "# 6. MODELING & ENSEMBLE\n",
    "# ============================================================\n",
    "\n",
    "# Calculate class weights (useful for imbalanced classification)\n",
    "classes = np.unique(y_train)\n",
    "class_to_index = {cls: idx for idx, cls in enumerate(classes)}\n",
    "y_train_numeric = np.array([class_to_index[cls] for cls in y_train])\n",
    "class_counts = np.bincount(y_train_numeric)\n",
    "total_samples = len(y_train_numeric)\n",
    "class_weights = total_samples / (len(classes) * class_counts)\n",
    "class_weights_dict = {cls: weight for cls, weight in zip(classes, class_weights)}\n",
    "print(\"Class weights:\", class_weights_dict)\n",
    "\n",
    "# Define a small set of models to use in the ensemble.\n",
    "models = {\n",
    "    'xgb': xgb.XGBClassifier(n_jobs=-1, eval_metric='logloss', objective='binary:logistic',\n",
    "                             tree_method='hist', random_state=42),\n",
    "    'knn': KNeighborsClassifier(n_neighbors=101, p=1),\n",
    "    'svm': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# Define an Optuna-based ensemble weight optimizer\n",
    "class OptunaWeights:\n",
    "    def __init__(self, n_trials=300, metric='auc', random_state=42):\n",
    "        self.n_trials = n_trials\n",
    "        self.metric = metric\n",
    "        self.random_state = random_state\n",
    "        self.weights = None\n",
    "        self.study = None\n",
    "        self.calibrated_threshold = 0.5\n",
    "\n",
    "    def fit(self, y_true, y_preds):\n",
    "        # y_preds is a list of prediction arrays\n",
    "        def objective(trial):\n",
    "            ws = [trial.suggest_uniform(f'w{i}', -1, 1) for i in range(len(y_preds))]\n",
    "            weighted = np.average(np.array(y_preds), axis=0, weights=ws)\n",
    "            score = roc_auc_score(y_true, weighted)\n",
    "            return score\n",
    "        self.study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=self.random_state))\n",
    "        self.study.optimize(objective, n_trials=self.n_trials, show_progress_bar=False)\n",
    "        self.weights = [self.study.best_trial.params[f'w{i}'] for i in range(len(y_preds))]\n",
    "        self.best_score = self.study.best_value\n",
    "\n",
    "    def predict(self, y_preds, normalize=True):\n",
    "        y_preds_np = [np.array(pred) for pred in y_preds]\n",
    "        weighted_pred = np.average(np.array(y_preds_np), axis=0, weights=self.weights)\n",
    "        if normalize:\n",
    "            weighted_pred = np.clip(weighted_pred, 0, 1)\n",
    "        return weighted_pred\n",
    "\n",
    "    def fit_predict(self, y_true, y_preds, normalize=True):\n",
    "        self.fit(y_true, y_preds)\n",
    "        return self.predict(y_preds, normalize=normalize)\n",
    "\n",
    "# Cross-validation with ensemble weighting\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_preds = np.zeros(len(X_train))\n",
    "test_preds = np.zeros(len(X_test))\n",
    "ensemble_scores = []\n",
    "weights_list = []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    X_tr, X_val = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    fold_val_preds = {}\n",
    "    fold_test_preds = {}\n",
    "    # Train each model and obtain predictions\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_val_pred = model.predict_proba(X_val)[:, 1]\n",
    "        y_test_pred = model.predict_proba(X_test)[:, 1]\n",
    "        fold_val_preds[name] = y_val_pred\n",
    "        fold_test_preds[name] = y_test_pred\n",
    "\n",
    "        print(f'{name} [Fold {fold}] AUC: {roc_auc_score(y_val, y_val_pred):.5f}')\n",
    "\n",
    "    # Optimize ensemble weights on validation fold predictions\n",
    "    optw = OptunaWeights(n_trials=300, random_state=42, metric='auc')\n",
    "    ensemble_val_pred = optw.fit_predict(y_val, list(fold_val_preds.values()))\n",
    "    oof_preds[val_idx] = ensemble_val_pred\n",
    "    fold_auc = roc_auc_score(y_val, ensemble_val_pred)\n",
    "    ensemble_scores.append(fold_auc)\n",
    "    weights_list.append(optw.weights)\n",
    "\n",
    "    # Get ensemble prediction for test set (average over folds)\n",
    "    model_test_preds = list(fold_test_preds.values())\n",
    "    ensemble_test_pred = optw.predict(model_test_preds)\n",
    "    test_preds += ensemble_test_pred / kf.n_splits\n",
    "\n",
    "    print(f'Ensemble [Fold {fold}] AUC: {fold_auc:.5f}')\n",
    "\n",
    "print(\"Overall Ensemble CV AUC:\", np.mean(ensemble_scores))\n",
    "print(\"Optimized Ensemble Weights (per fold):\", weights_list)\n",
    "\n",
    "# ============================================================\n",
    "# 7. FINAL SUBMISSION\n",
    "# ============================================================\n",
    "\n",
    "submission[target] = test_preds\n",
    "submission.to_csv('submission_pure.csv', index=True)\n",
    "\n",
    "# Optionally, save out-of-fold predictions for future stacking/analysis.\n",
    "oof_df = pd.DataFrame(oof_preds, index=X_train.index, columns=[target])\n",
    "oof_df.to_csv('oof_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3cd254-4275-4870-ba09-17c7ab16ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. SETUP & IMPORTS\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA LOADING\n",
    "# =============================================================================\n",
    "# Load the competition training data and external dataset (\"original\")\n",
    "train = pd.read_csv('/kaggle/input/playground-series-s5e3/train.csv', index_col='id')\n",
    "original = pd.read_csv('/kaggle/input/rainfall-prediction-using-machine-learning/Rainfall.csv')\n",
    "\n",
    "# Clean external dataset: strip whitespace from column names and convert rainfall\n",
    "original.columns = [col.strip() for col in original.columns]\n",
    "original['rainfall'] = original['rainfall'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# Load sample submission (our own submission to be blended later)\n",
    "submission = pd.read_csv('/kaggle/input/playground-series-s5e3/sample_submission.csv')\n",
    "\n",
    "# =============================================================================\n",
    "# 3. EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# =============================================================================\n",
    "# --- 3.1 Target Distribution: Pie Charts ---\n",
    "\n",
    "def plot_rainfall_distribution(data, title, ax, colors=None, shadow=True, startangle=90):\n",
    "    \"\"\"\n",
    "    Create an enhanced pie chart showing the distribution of the target variable 'rainfall'.\n",
    "    \"\"\"\n",
    "    target = 'rainfall'\n",
    "    data_counts = data[target].value_counts().sort_index()\n",
    "    labels = ['No Rain (0)', 'Rain (1)']\n",
    "    sizes = [data_counts.get(0, 0), data_counts.get(1, 0)]\n",
    "    \n",
    "    # Set default colors if not provided\n",
    "    if colors is None:\n",
    "        colors = ['#3498db', '#e74c3c']  # Blue for no rain, Red for rain\n",
    "    \n",
    "    # Slightly \"explode\" the rain slice to emphasize\n",
    "    explode = (0, 0.1)\n",
    "    \n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        sizes,\n",
    "        explode=explode,\n",
    "        labels=labels,\n",
    "        colors=colors,\n",
    "        autopct='%1.1f%%',\n",
    "        shadow=shadow,\n",
    "        startangle=startangle,\n",
    "        wedgeprops={'edgecolor': 'w', 'linewidth': 1}\n",
    "    )\n",
    "    \n",
    "    # Customize text appearance\n",
    "    for text in texts:\n",
    "        text.set_fontsize(12)\n",
    "        text.set_fontweight('bold')\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(11)\n",
    "        autotext.set_fontweight('bold')\n",
    "        autotext.set_color('white')\n",
    "        \n",
    "    # Add total sample info\n",
    "    ax.text(0, -1.2, f\"Total samples: {len(data)}\", ha='center', fontsize=10,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc='#f0f0f0', ec='gray'))\n",
    "    ax.axis('equal')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Create subplots for the train and external datasets\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "custom_colors = [['#3498db', '#e74c3c'], ['#2ecc71', '#9b59b6']]\n",
    "plot_rainfall_distribution(train, \"Train Dataset: Rainfall Distribution\", axes[0], colors=custom_colors[0])\n",
    "plot_rainfall_distribution(original, \"Original Dataset: Rainfall Distribution\", axes[1], colors=custom_colors[1])\n",
    "fig.suptitle('Comparison of Rainfall Distribution Across Datasets', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# --- 3.2 Numerical Feature Distributions ---\n",
    "# Identify continuous columns (non-object and with more than 2 unique values)\n",
    "cont_cols = [col for col in train.columns if train[col].dtype != 'O' and train[col].nunique() > 2]\n",
    "n_rows = len(cont_cols)\n",
    "\n",
    "# Create a subplot grid with two columns (Train and Original) per feature\n",
    "fig, axs = plt.subplots(n_rows, 2, figsize=(14, 4.5 * n_rows), dpi=100)\n",
    "if n_rows == 1:\n",
    "    axs = np.array([axs])  # ensure 2D array\n",
    "\n",
    "for i, col in enumerate(cont_cols):\n",
    "    # Violin plot for train dataset\n",
    "    sns.violinplot(x='rainfall', y=col, data=train, ax=axs[i, 0],\n",
    "                   palette=custom_colors, inner='quartile', linewidth=1.5, cut=0)\n",
    "    axs[i, 0].set_title(f'{col.title()} Distribution by Rainfall (Train)', fontsize=14, fontweight='bold', pad=10)\n",
    "    axs[i, 0].set_xlabel('Rainfall', fontsize=12, labelpad=10)\n",
    "    axs[i, 0].set_ylabel(col.title(), fontsize=12, labelpad=10)\n",
    "    axs[i, 0].set_xticklabels(['No Rain (0)', 'Rain (1)'])\n",
    "    sns.despine(ax=axs[i, 0])\n",
    "    \n",
    "    # Violin plot for original dataset\n",
    "    sns.violinplot(x='rainfall', y=col, data=original, ax=axs[i, 1],\n",
    "                   palette=custom_colors, inner='quartile', linewidth=1.5, cut=0)\n",
    "    axs[i, 1].set_title(f'{col.title()} Distribution by Rainfall (Original)', fontsize=14, fontweight='bold', pad=10)\n",
    "    axs[i, 1].set_xlabel('Rainfall', fontsize=12, labelpad=10)\n",
    "    axs[i, 1].set_ylabel(col.title(), fontsize=12, labelpad=10)\n",
    "    axs[i, 1].set_xticklabels(['No Rain (0)', 'Rain (1)'])\n",
    "    sns.despine(ax=axs[i, 1])\n",
    "    \n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. EXTERNAL BLEND OF SUBMISSIONS\n",
    "# =============================================================================\n",
    "# The following function computes ensemble predictions using Arithmetic, Geometric, or Harmonic Mean.\n",
    "\n",
    "def ensemble_mean(sub_list, cols, mean=\"AM\"):\n",
    "    \"\"\"\n",
    "    Computes the mean (Arithmetic, Geometric, or Harmonic) over a list of submission dataframes.\n",
    "    \n",
    "    Parameters:\n",
    "        sub_list (list of pd.DataFrame): List of submission dataframes with same target columns.\n",
    "        cols (list): List of target column names.\n",
    "        mean (str): \"AM\" for arithmetic mean, \"GM\" for geometric mean, \"HM\" for harmonic mean.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A blended submission dataframe.\n",
    "    \"\"\"\n",
    "    sub_out = sub_list[0].copy()\n",
    "    if mean == \"AM\":\n",
    "        for col in cols:\n",
    "            sub_out[col] = sum(df[col] for df in sub_list) / len(sub_list)\n",
    "    elif mean == \"GM\":\n",
    "        for df in sub_list[1:]:\n",
    "            for col in cols:\n",
    "                sub_out[col] *= df[col]\n",
    "        for col in cols:\n",
    "            sub_out[col] = sub_out[col] ** (1 / len(sub_list))\n",
    "    elif mean == \"HM\":\n",
    "        for col in cols:\n",
    "            sub_out[col] = len(sub_list) / sum(1 / df[col] for df in sub_list)\n",
    "    return sub_out\n",
    "\n",
    "# Read external submission files\n",
    "sub_ext1 = pd.read_csv(\"/kaggle/input/ps3e5-ensemble-ancillary/sub_exxt_vyacheslavbolotin_v114_95655.csv\")\n",
    "sub_ext2 = pd.read_csv(\"/kaggle/input/ps3e5-ensemble-ancillary/sub_ext_cdeotte_v1_95628.csv\")\n",
    "sub_ext3 = pd.read_csv(\"/kaggle/input/ps3e5-ensemble-ancillary/sub_ext_act18l_v17_91522.csv\")\n",
    "sub_ext4 = pd.read_csv(\"/kaggle/input/ps3e5-ensemble-ancillary/sub_ext_itasps_v23_91499.csv\")\n",
    "sub_ext5 = pd.read_csv(\"/kaggle/input/ps3e5-ensemble-ancillary/sub_ext_act18l_v9_91043.csv\")\n",
    "sub_ext6 = pd.read_csv(\"/kaggle/input/ps3e5-ensemble-ancillary/sub_ext_act18l_v9_90935.csv\")\n",
    "sub_ext7 = pd.read_csv(\"/kaggle/input/ps3e5-ensemble-ancillary/sub_ext_vyacheslavbolotin_v28_90581.csv\")\n",
    "sub_ext8 = pd.read_csv(\"/kaggle/input/ps3e5-ensemble-ancillary/sub_ext_vyacheslavbolotin_v20_89702.csv\")\n",
    "sub_ext9 = pd.read_csv(\"/kaggle/input/ps3e5-ensemble-ancillary/sub_ext_vyacheslavbolotin_v53_89568.csv\")\n",
    "sub_ext10 = pd.read_csv(\"/kaggle/input/ps3e5-ensemble-ancillary/sub_ext_samanyuk_v6_89541.csv\")\n",
    "\n",
    "# Combine external submissions with our own submission.\n",
    "# (Adjust the list as needed. Here we include a total of 7 submissions.)\n",
    "sub_list = [sub_ext1, sub_ext2, sub_ext3, sub_ext4, sub_ext7, sub_ext10, submission]\n",
    "\n",
    "target_columns = ['rainfall']\n",
    "\n",
    "# Scale predictions using MinMaxScaler (this makes results comparable)\n",
    "for i in range(len(sub_list)):\n",
    "    scaler = MinMaxScaler()\n",
    "    for col in target_columns:\n",
    "        sub_list[i][col] = scaler.fit_transform(sub_list[i][col].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Define weights for each submission.\n",
    "# In this example, we weight our own submission more heavily.\n",
    "weights = np.square([7, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "# (Optional) Repeat submissions according to weight to simulate weighted averaging\n",
    "if len(sub_list) == len(weights):\n",
    "    weighted_list = [item for sub, weight in zip(sub_list, weights) for item in [sub] * weight]\n",
    "\n",
    "# Compute ensemble submissions using different means\n",
    "am_submission = ensemble_mean(weighted_list, target_columns, mean=\"AM\")\n",
    "gm_submission = ensemble_mean(weighted_list, target_columns, mean=\"GM\")\n",
    "hm_submission = ensemble_mean(weighted_list, target_columns, mean=\"HM\")\n",
    "\n",
    "# Save the blended submissions to CSV files\n",
    "am_submission.to_csv('submission_blended_am.csv', index=False)\n",
    "gm_submission.to_csv('submission_blended_gm.csv', index=False)\n",
    "hm_submission.to_csv('submission_blended_hm.csv', index=False)\n",
    "\n",
    "print(\"Blended submissions saved (Arithmetic, Geometric, and Harmonic).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
